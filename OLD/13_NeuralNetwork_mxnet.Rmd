---
title: "機械学習４：ニューラルネットワーク/ディープラーニング"
author: ""
date: "`r Sys.Date()`"
output:
 html_document:
    css : R_style.css
    self_contained: true   # 画像などの埋め込み 
    #theme: cosmo
    highlight: haddock     # Rスクリプトのハイライト形式
    #code_folding: 'hide'  # Rコードの折りたたみ表示を設定
    toc: true
    toc_depth: 2           # 見出しの表示とその深さを指定
    toc_float: true        # 見出しを横に表示し続ける
    number_sections: true # 見出しごとに番号を振る
    # df_print: paged        # head()の出力をnotebook的なものに（tibbleと相性良）
    latex_engine: xelatex  # zxjatypeパッケージを使用するために変更
    fig_height: 4          # 画像サイズのデフォルトを設定
    fig_width: 6           # 画像サイズのデフォルトを設定
    dev: png
classoption: xelatex,ja=standard
editor_options: 
  chunk_output_type: console
---


```{r include = FALSE}
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
```

$$
% 定義
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
$$

# ニューラルネットワーク

## パーセプトロン

### 生体ニューロン

生物の脳神経には多数のニューロン（神経細胞）があり，それらが結合してネットワークを形成している。生物のニューロン（生体ニューロン）ではニューロンの樹状突起が他のニューロンのシナプスから信号（神経伝達物質を受容することで発生する電気信号）を受け取り，閾値を超える電気信号を受け取ると別のニューロンに信号を送るような仕組みになっている。

シナプスと樹状突起の結合の強さ（神経伝達物質の放出量）はそれぞれ異なり，よく使う神経回路はシナプス結合が強くなり，そうすることで学習が進行する。



<center><img src="13_NeuralNetwork.assets/1553697913924.png" width=70%></center>

### 人工ニューロン

こうしたニューロンの仕組みを数理モデルにした人工ニューロン（ユニットと呼ばれる）を用いてネットワークを作ったものが**ニューラルネットワーク**（neural network）である。以下の図(a)は1つのユニットを，図(b)はニューラルネットワークの一例を示している。

![](13_NeuralNetwork.assets/1553736366556.png)

ユニットにはさまざまな入力$x_i$が異なる結合の強さ$w_i$で入ってきており，その重み付き和
$$
u = \sum_{i = 1}^p x_i w_i
$$
が総入力となる。$u$は**活性**（activation）とも呼ばれる。活性に**バイアス**（bias）と呼ばれる定数項$b$を加えて**活性化関数**（activation function）を通したものがこのユニットからの出力$z$となる。
$$
z = f(u + b) = f\left(\sum_{i = 1}^p x_i w_i + b \right)
$$
生体ニューロンにおいて総入力がある閾値を超えたときに信号を出力することを再現するのが活性化関数であり，**階段関数**（step function）
$$
\theta(x) = 
\begin{cases}
1 & (x \geq b)\\
0 & (x < b)
\end{cases}
$$
や**シグモイド関数**（sigmoid function）
$$
\sigma(x) = \frac{1}{1 + \exp(-x)}
$$
を使用する。



```{r, fig.height=4, fig.width=8, echo=F}
library(gridExtra)
library(tidyverse)

# 階段関数
step = function(x, b) ifelse(x >= -b, 1, 0)
# シグモイド関数
sigmoid = function(x) 1 / (1 + exp(-x))


# データ生成
x = -100:100 * 0.1

# plot
g1 <- ggplot(data_frame(x, y = step(x, 0)),
             aes(x = x, y = y))+
  geom_line(color = "dodgerblue")+
  labs(y = expression(y), x = expression(x), title = "階段関数")

g2 <- ggplot(data_frame(x, y = sigmoid(x)),
             aes(x = x, y = y))+
  geom_line(color = "dodgerblue")+
  labs(y = expression(y), x = expression(x), title = "シグモイド関数")

grid.arrange(g1, g2, ncol = 2)
```


重み$w_i$とバイアス$b$はデータから学習させる。その際には，まず重みの初期値としてランダムに設定し，その後は誤差を下げる方向に重みの更新を繰り返していくように推定していく。


## Rで実践

### データの準備

`{carData}`パッケージに含まれるタイタニック号の乗客データ`TitanicSurvival`を使う。

```{r}
# 準備
library(tidyverse)
set.seed(666)

# データ読み込み
library(carData)
data("TitanicSurvival")
head(TitanicSurvival)
```

このデータセットは1912年のタイタニック号の沈没事故の乗客の生死に関するデータで，次の変数が含まれている

- `survived`：生存したかどうか
- `sex`：性別
- `age`：年齢（1歳に満たない幼児は小数）。263の欠損値を含む。
- `passengerClass`：船室の等級

このデータセットには欠損値が含まれているため，まず欠損値を除去する

```{r}
# NA（欠損値）を含む行を削除
tita <- na.omit(TitanicSurvival)
```

そしてデータを学習用・テスト用に分割する。

```{r}
# ID列を追加
df = tita %>% rownames_to_column("ID")

# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)

# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")

# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
```

### ニューラルネット

中間層が1層のニューラルネットワークは`{nnet}`パッケージで実行することができる。

引数に指定している`size`は中間層のユニット数，`decay`は重みを更新するときの減衰率である。

<!-- nnetではBroyden, Fletcher, Goldfarb and Shanno(1970)の準ニュートン法を用いてパラメータを推定する。 -->

```{r}
# 単一中間層
library(nnet)
clf_nnet <- nnet(survived ~ . , data = train,
                 size = 2, decay = 0.1)
```


```{r}
# 予測
y_pred = predict(clf_nnet, test, type = "class")

# 混同行列
table(test$survived, y_pred)

# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred, y_true = test$survived)
```



# ディープラーニング

## 登場の経緯

ディープラーニングは多層ニューラルネットワークを用いた手法である。中間層を多層化するとニューラルネットの表現力が向上し，複雑な識別境界を描くことができる。

しかし，1990年代~2000年代には

1. **勾配消失**：活性化関数にシグモイド関数を用いて多層にすると，浅い層のパラメータが出力に影響を与えなくなって更新されなくなり学習が困難になる
2. **過学習**：表現力が向上する一方で過学習しやすくなる
3. **計算コスト**：並列計算が必要で，従来のCPUはこのタスクに向かない

といった問題が解決できなかった。

その後研究が進み，これらの問題は次のように解決できるようになった。

1. 勾配消失 → **ReLU**（rectified linear unit，活性化関数に正規化線形関数を使ったユニット）を使用する
   - $f(x) = \max(x, 0)$という活性化関数を使用する
2. 過学習 → **Dropout**：中間層のニューロンを学習のたびにいくつかランダムに無効化しつつ学習する
3. 計算コスト → **General Purpose GPU（GPGPU）**：並列演算用の処理装置の登場

そして2012年に画像認識のコンテストILSVRC2012においてディープラーニングを使用したチームが他のチームを圧倒する成績を出したことでその性能が注目されるようになり，ディープラーニングのブームが始まった。現在もディープラーニング（AI，人工知能）ブームは続いている。

## Rで実践

### データの用意

```{r}
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)

# 乱数の種を固定
set.seed(0)
```

[MNIST database](http://yann.lecun.com/exdb/mnist/)の手書き文字の画像データを使う。

```{r}
# MNISTデータのダウンロード
if (!dir.exists('data')) { # もしdataディレクトリがないなら作成
    dir.create('data')
}
if (!file.exists('data/train.csv')) { # もしdataディレクトリにtrain.csvがないならダウンロード
    download.file(url='https://raw.githubusercontent.com/wehrley/Kaggle-Digit-Recognizer/master/train.csv',
                  destfile='data/train.csv')
}
```

ダウンロードしたらデータの読み込みと前処理を行う

```{r}
# データの読み込み ---------------
mnist <- read.csv('data/train.csv') %>% 
  # 使うデータを半分に減らす（スペックの高いPCを使う場合はこの処理は不要）
  sample_frac(size = 0.5)

# testとtrainに分割 --------------
# ID列を追加
df = mnist %>% rownames_to_column("ID")

# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)

# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")

# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)


# その他の前処理 ----------------
# 行列型に変換
train <- data.matrix(train)
test <- data.matrix(test)

# 教師データ（目的変数）
train_y <- train[,1]
test_y <- test[,1]

# 特徴量（説明変数）
train_features <- train[,-1]
test_features <- test[,-1]

# 行列を転置
train_X <- t(train_features)
test_X <- t(test_features)

# 0から1の値になるよう正規化
train_X <- train_X / 255
test_X <- test_X / 255
```

このデータは次のような28 x 28ピクセルの手書き数字データになっている。

```{r}
# 扱っているデータを覗く
i = 100 # 任意の番号のデータ
pixels = matrix(train_X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((0:255)/255), 
      xlab="", ylab="", main=paste("Label for this image:", train_y[i]))
```

目的変数（教師データ）は0~9のラベルで，それぞれ3000レコード程度ある

```{r}
# 教師データのラベル
table(train_y)
```


### DNN

ディープラーニングを行うには`{mxnet}`パッケージを使用する。インストール方法が少し面倒だが次のように書いてインストールすればよい
（参考：[mxnetの公式サイト](http://mxnet.incubator.apache.org/versions/master/install/index.html)）。

```{r, eval=F}
# mxnetのインストール
cran <- getOption("repos")
cran["dmlc"] <- "https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/"
options(repos = cran)
install.packages("mxnet")
```


まずネットワーク構造を決める。以下では多層ニューラルネットワーク（Deep Neural Network: DNN）を使ってみる。

これはこれまで図示してきたもののようにユニット同士が全部結合されている全結合層（fully connected layer）を使うというシンプルなネットワーク構造になっている。

```{r}
library(mxnet)
# ネットワーク構造の宣言： Deep Neural Network

data <- mx.symbol.Variable("data")
# 第1中間層
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128) # ユニット数128
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")  # ReLUの活性化関数
# 第2中間層
fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64) # ユニット数64
act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu") # ReLUの活性化関数
# 第3中間層
fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10) # ユニット数10
softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm") # softmax活性化関数（シグモイド関数の多値バージョン）
```


```{r}
# 計算に使うデバイスを設定
devices <- mx.cpu()
```

```{r}
# ニューラルネットを訓練
mx.set.seed(0)
model_dnn <- mx.model.FeedForward.create(softmax, X=train_X, y=train_y,
                                     ctx=devices, num.round=10, array.batch.size=100,
                                     learning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,
                                     initializer=mx.init.uniform(0.07),
                                     epoch.end.callback=mx.callback.log.train.metric(100))
```


```{r}
# 予測
preds <- predict(model_dnn, test_X)
predicted_labels <- max.col(t(preds)) - 1

# 混同行列
table(test_y, predicted_labels)

# 正解率
Accuracy(y_pred = predicted_labels,
         y_true = test_y)
```





### CNN

次は畳み込みニューラルネットワーク（Convolutional Neural Network: CNN）という画像認識を得意とするアーキテクチャを使って予測を行う。

CNNは次のような層を使うのが特徴である

- **畳み込み層**（convolutional layer）
    - ある領域のデータを
- **プーリング層**（pooling layer）
    - 例えばmax poolingの場合，各セグメントでの最大値を採用する（最も強く現れている特徴の情報を使う）


```{r}
# ネットワーク構造を宣言：CNN -----

data <- mx.symbol.Variable('data')
# 第1畳み込み層: 20 フィルター
conv1 <- mx.symbol.Convolution(data=data, kernel=c(5,5), num_filter=20) # 畳み込み層
tanh1 <- mx.symbol.Activation(data=conv1, act_type="tanh") # 活性化関数はtanh関数（シグモイド関数に似たS字関数）
pool1 <- mx.symbol.Pooling(data=tanh1, pool_type="max", # プーリング層（max pooling）
                          kernel=c(2,2), stride=c(2,2))
# 第1畳み込み層: 50 フィルター
conv2 <- mx.symbol.Convolution(data=pool1, kernel=c(5,5), num_filter=50)
tanh2 <- mx.symbol.Activation(data=conv2, act_type="tanh")
pool2 <- mx.symbol.Pooling(data=tanh2, pool_type="max",
                          kernel=c(2,2), stride=c(2,2))
# 結果をベクトルに変換
flatten <- mx.symbol.Flatten(data=pool2)

# 第1全結合層
fc1 <- mx.symbol.FullyConnected(data=flatten, num_hidden=500)
tanh3 <- mx.symbol.Activation(data=fc1, act_type="tanh")
# 第2全結合層（出力層）
fc2 <- mx.symbol.FullyConnected(data=tanh3, num_hidden=10)
lenet <- mx.symbol.SoftmaxOutput(data=fc2)
```

```{r}
# データ型をarrayに変える
train_array <- train_X
dim(train_array) <- c(28, 28, 1, ncol(train_X))
test_array <- test_X
dim(test_array) <- c(28, 28, 1, ncol(test_X))
```

```{r}
# 計算に使うデバイスを設定

# CPUの場合
devices <- mx.cpu()

# GPUを使う場合
```

```{r}
# ネットワークの学習
mx.set.seed(0)
model_cnn <- mx.model.FeedForward.create(lenet, X=train_array, y=train_y,
            ctx=devices, num.round=10, array.batch.size=100,
            learning.rate=0.05, momentum=0.9, wd=0.00001,
            eval.metric=mx.metric.accuracy,
            epoch.end.callback=mx.callback.log.train.metric(100))
```


```{r}
# 予測
preds <- predict(model_cnn, test_array)
predicted_labels <- max.col(t(preds)) - 1

# 混同行列
table(test_y, predicted_labels)

# 正解率
Accuracy(y_pred = predicted_labels,
         y_true = test_y)
```




