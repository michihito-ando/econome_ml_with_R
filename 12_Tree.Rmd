---
title: "Rで学ぶ計量経済学と機械学習 8<br> <br> 機械学習３：決定木/ランダムフォレスト"
author: "安藤道人（立教大学）　三田匡能 (株式会社 GA technologies)"
date: "`r Sys.Date()`"
output:
 html_document:
    css : R_style.css
    self_contained: true   # 画像などの埋め込み 
    #theme: cosmo
    highlight: haddock     # Rスクリプトのハイライト形式
    #code_folding: 'hide'  # Rコードの折りたたみ表示を設定
    toc: true
    toc_depth: 2           # 見出しの表示とその深さを指定
    toc_float: true        # 見出しを横に表示し続ける
    number_sections: true # 見出しごとに番号を振る
    # df_print: paged        # head()の出力をnotebook的なものに（tibbleと相性良）
    latex_engine: xelatex  # zxjatypeパッケージを使用するために変更
    fig_height: 4          # 画像サイズのデフォルトを設定
    fig_width: 6           # 画像サイズのデフォルトを設定
    dev: png
classoption: xelatex,ja=standard
editor_options: 
  chunk_output_type: console
---


```{r include = FALSE}
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
```

$$
% 定義
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
$$

# 決定木

## ツリーモデル

**決定木**（decision tree）は説明変数の値に応じて条件分岐を繰り返して木のような構造をとるアルゴリズムである。

以下の図は$x_1,x_2$という説明変数を使って目的変数$y$の分類を決定木によって行うときのイメージである。左側の図が木構造のモデルの様子を表しており，まず「$x_1<a$」であるかどうかによって分岐し，続いて$x_1<a$がYesであったほうの枝において「$x_2<b$」であるかどうかで分岐するようにし，最終的に予測を行う。右側の図はこの条件分岐によって生み出される識別境界を示している。木はノードとリンクから構成され，分岐の結節点を**ノード**（node），ノードとノードを結ぶ線をリンクという。

![](12_Tree.assets/1552647653428.png)





## 学習規則

線形回帰ではパラメータ$\beta$を推定して学習を行っていたが，決定木では最適な分割を推定することで学習を行う。

決定木では**不純度**（impurity）と呼ばれる指標を用いて最適な分割を見つける。不純度には**ジニ係数**（gini index）
$$
I(t) = 1 - \sum_{i=1}^k {p(i|t)}^2
$$
などが使われる。ここで$t$はノード，$i$は目的変数の値で，$p(i|t)$はノード$t$の領域に属する学習用データのサンプルサイズ$N(t)$のうち目的変数が$i$の値をとるデータ数$N_i(t)$の割合

$$
p(i|t) = \frac{N_i(t)}{N(t)}
$$
である。あるノードの領域に属するサンプルが全て同じクラスに属する場合（異なるクラスのサンプルが混じらずに綺麗に分割できている場合）にはその不純度は0になる。

そして，ノード$t$の不純度$I(t)$やサンプルサイズ$N(t)$と，ノード$t$から左右に分割される子ノード$t_{L},t_{R}$の不純度$I(t_L),I(t_R)$と子ノードの領域に属するサンプルサイズ$N(t_L),N(t_R)$を用いて算出される**情報利得**（information gain）
$$
I(t) - \left\{\frac{N(t_L)}{N(t)} I(t_{L}) + \frac{N(t_R)}{N(t)} I(t_{R})\right\}
$$
を最大化する分割，すなわち，分割後の子ノードの不純度を最も下げるような分割を採用する。


<!-- ## 正則化 -->

<!-- あまりにも細かく分割しすぎるとかえって過学習を招くため，ほどよいところで分割を止めなければならない。 -->


## Rで実践

### データの準備

`{carData}`パッケージに含まれるタイタニック号の乗客データ`TitanicSurvival`を使う。

```{r}
# 準備
library(tidyverse)
set.seed(666)

# データ読み込み
library(carData)
data("TitanicSurvival")
head(TitanicSurvival)
```

このデータセットは1912年のタイタニック号の沈没事故の乗客の生死に関するデータで，次の変数が含まれている

- `survived`：生存したかどうか
- `sex`：性別
- `age`：年齢（1歳に満たない幼児は小数）。263の欠損値を含む。
- `passengerClass`：船室の等級

このデータセットには欠損値が含まれているため，まず欠損値を除去する

```{r}
# NA（欠損値）を含む行を削除
titanic <- na.omit(TitanicSurvival)
```

そしてデータを学習用・テスト用に分割する。

```{r}
# ID列を追加
df = titanic %>% rownames_to_column("ID")

# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)

# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")

# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
```


### 決定木の実行

決定木の実行には`{rpart}`パッケージを使う。決定木の作図用に`{partykit}`もインストールしておく。

```{r, eval=F}
# パッケージのインストール
install.packages(c("rpart", "partykit"))
```

`rpart()`関数を用いて決定木を実行できる。`lm()`関数と同様に，`formula`の引数にモデルの構造すなわち被説明変数・説明変数の関係を記述し`data`の引数にデータのオブジェクトを入れる。


```{r}
# パッケージの読み込み
library(rpart)

# rpartを実施し、titanic_treeに結果を保存
# lmと同様、y ~ x。"." は「残りの変数全部」を意味する
titanic_tree <- rpart(survived ~ . , data = train)  
```

ここでは表示しないが、結果は`summary()`や`print()`で表示できる。

```{r  eval=F}
summary(titanic_tree)
print(titanic_tree)
```


`{partykit}`パッケージを使えば決定木を描くことができる。

```{r, fig.height=6, fig.width=10}
# パッケージの読み込み
library(partykit)

# 決定木のプロット
plot(as.party(titanic_tree))
```

まず性別で分岐し，男性の場合は年齢が9.5歳未満だと生存する割合が高くなるが，9.5歳以上だと生存する割合は低くなっている。
女性の場合，船室の等級が1stや2ndであれば生存する割合が極めて高いが，3rdの場合は年齢が高いほど生存する割合は低くなっていることがわかる。

決定木はこのようにデータの解釈・説明を行うことができる。そのため線形回帰などと並んでマーケティングの現場などビジネスにおいても使われる。

次に、実際の値と予測値を比べる。正解率を計算するために`Accuracy()`を使用するので、`MLmetrics`パッケージを読み込んでおく

```{r}
library(MLmetrics)
```

まずは訓練データで検証する。
```{r}
# 予測値を保存
y_pred_train <- predict(titanic_tree, train, type = "class")

# 混同行列:実際の値と予測値を比べる
table(train$survived, y_pred_train)

# 訓練データでの正解率
Accuracy(y_pred = y_pred_train, y_true = train$survived)
```

次いで、テストデータで検証する。
```{r}
# 予測値を保存
y_pred_test <- predict(titanic_tree, test, type = "class")

# 混同行列:実際の値と予測値を比べる
table(test$survived, y_pred_test)

# 訓練データでの正解率
Accuracy(y_pred = y_pred_test, y_true = test$survived)
```

```{r}
# 予測
y_pred = predict(titanic_tree, test, type = "class")

# 混同行列:実際の値と予測値を比べる
table(test$survived, y_pred)
```

訓練データでの正解率よりも低いものの、76%程度の正解率となっている。

# ランダムフォレスト　

決定木は予測モデルの解釈可能性が高いものの，単一の決定木では予測精度はあまり高くない。

複数の予測モデルの結果を統合して答えを出す手法を**アンサンブル学習**（ensemble learning）という。決定木は予測精度が目的であるときには単体で使われることはほとんどなく，アンサンブルして使われる事が多い。

決定木のアンサンブル学習で最も有名な手法が**ランダムフォレスト**（random forest）である。

ランダムフォレストでは複数の決定木を使うが，その際に決定木間の相関を減らすために学習用データのサンプルから重複を許して無作為抽出（**ブートストラップサンプリング**）したデータを用いる。

さらに終端の(最後の）ノード以外の各ノードにおいて、あらかじめ決めておいた数だけの説明変数をランダムに選択して使用する。(なお、すべての説明変数を使う方法は**バギング**と呼ばれる。ランダムフォレストはバギングの改良版である)

決定木の結果を多数決（分類問題の場合）あるいは平均（回帰問題の場合）したものを最終的な予測値として採用する。


## Rで実践

`{randomForest}`パッケージを使う。

```{r, eval=F}
# パッケージのインストール
install.packages("randomForest")
```


```{r}
# パッケージの読み込み
library(randomForest)

# randomForestを実施し、titanic_RFに保存。
# lmと同様、y ~ x。"." は「残りの変数全部」を意味する
# デフォルトの設定をそのまま使う。
titanic_RF <- randomForest(survived ~ . , data = train)
```

結果の概略は`print`によって表示できる。

```{R}
print(titanic_RF)
```

（なお、ここでの`OOB estimate of error rate`とは、**Out-of-bag(OOB)による誤分類率の推定値**である。これは、ブートストラップサンプルによる決定木の生成では、平均的に訓練データの観測値の約3分の2を用いられ、残りの約3分の1の観測値は用いられない（out-of-bag)であることを利用した誤分類率の推定値である。つまり、個々の訓練データの観測値について、その観測値が使われなかった（out-of-bagであった）ブートストラップサンプルによる決定木の予測値（の多数決）を用いて予測を評価し、そこから全体の誤分類率を計算している。）

まずは訓練データで検証する。

```{r}
# 予測値を保存
y_pred_train = predict(titanic_RF, train, type = "class")

# 混同行列
table(train$survived, y_pred_train)

# 正解率
Accuracy(y_pred = y_pred_train, y_true = train$survived)
```

単一の決定木よりも正解率は高くなっている。

次いで、テストデータで検証する。

```{r}
# 予測
y_pred_test = predict(titanic_RF, test, type = "class")

# 混同行列
table(test$survived, y_pred_test)

# 正解率
Accuracy(y_pred = y_pred_test, y_true = test$survived)
```

こちらも、単一の決定木よりも正解率は高くなっている。

また、ランダムフォレストでは，`$importance`でジニ係数の減少に基づいて算出される**変数重要度**（variable importance）を確認することができる。これは構築した予測モデルにおいてそれぞれの説明変数がどの程度予測に寄与しているのかを教えてくれる。

```{r}
# 変数重要度はimportanceとして保存されている
titanic_RF$importance

# importance関数でも表示できる。
importance(titanic_RF)
```

最も予測に寄与している説明変数は性別であることが分かる。

# 参考文献
James et al.(2018)『Rによる統計的学習入門』8章「木に基づく方法」

平井有三(2012)『はじめてのパターン認識』第11章「識別器の組み合わせによる性能強化」

[Package `randomForest`](https://cran.r-project.org/web/packages/randomForest/randomForest.pdf)

# 参考ウェブサイト&スライド
[初心者の初心者による初心者のための決定木分析](https://qiita.com/3000manJPY/items/ef7495960f472ec14377)

濵田晃一(2010)「[はじめてでもわかる RandomForest 入門－集団学習による分類・予測 －](https://www.slideshare.net/hamadakoichi/randomforest-web)」

渡部斉(2015)「[ランダムフォレスト](
https://www.slideshare.net/HitoshiHabe/ss-58784421)」


<!-- ```{r} -->
<!-- library(caret) -->
<!-- clf_RF <- train(form = survived ~ . , # survivedを目的変数に，残りのすべての列を説明変数にする -->
<!--                 data = train, -->
<!--                 method = "rf", # 決定木を指定 -->
<!--                 trControl = trainControl(method = "cv"), -->
<!--                 # tuneGrid = expand.grid(cp = 1:10 * 0.03)) -->
<!--                 tuneLength = 10) -->
<!-- clf_RF -->
<!-- ``` -->


<!-- ```{r} -->
<!-- library(caret) -->
<!-- clf_tree <- train(form = survived ~ . , # survivedを目的変数に，残りのすべての列を説明変数にする -->
<!--                   data = train, -->
<!--                   method = "rpart", # 決定木を指定 -->
<!--                   trControl = trainControl(method = "cv"), -->
<!--                   # tuneGrid = expand.grid(cp = 1:10 * 0.03), -->
<!--                   tuneLength = 10) -->
<!-- clf_tree -->

<!-- # 決定木のプロット -->
<!-- library(partykit) -->
<!-- plot(as.party(clf_tree$finalModel)) -->
<!-- ``` -->



