---
title: "Rで学ぶ計量経済学と機械学習 13<br> <br> 機械学習４：：ニューラルネットワーク/ディープラーニング"
author: "安藤道人（立教大学）　三田匡能 (株式会社 GA technologies)"
date: "`r Sys.Date()`"
output:
 html_document:
    css : R_style.css
    self_contained: true   # 画像などの埋め込み 
    #theme: cosmo
    highlight: haddock     # Rスクリプトのハイライト形式
    #code_folding: 'hide'  # Rコードの折りたたみ表示を設定
    toc: true
    toc_depth: 2           # 見出しの表示とその深さを指定
    toc_float: true        # 見出しを横に表示し続ける
    number_sections: true # 見出しごとに番号を振る
    # df_print: paged        # head()の出力をnotebook的なものに（tibbleと相性良）
    latex_engine: xelatex  # zxjatypeパッケージを使用するために変更
    fig_height: 4          # 画像サイズのデフォルトを設定
    fig_width: 6           # 画像サイズのデフォルトを設定
    dev: png
classoption: xelatex,ja=standard
editor_options: 
  chunk_output_type: console
---


```{r include = FALSE}
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
```

$$
% 定義
\newcommand{\argmin}{\mathop{\rm arg~min}\limits}
$$

# ニューラルネットワーク

## パーセプトロン

### 生体ニューロン

生物の脳神経には多数のニューロン（神経細胞）があり，それらが結合してネットワークを形成している。生物のニューロン（生体ニューロン）ではニューロンの樹状突起が他のニューロンのシナプスから信号（神経伝達物質を受容することで発生する電気信号）を受け取り，閾値を超える電気信号を受け取ると別のニューロンに信号を送るような仕組みになっている。

シナプスと樹状突起の結合の強さ（神経伝達物質の放出量）はそれぞれ異なり，よく使う神経回路はシナプス結合が強くなり，そうすることで学習が進行する。

<center><img src="13_NeuralNetwork.assets/neuron.png" width=70%></center>
画像出所：http://nkdkccmbr.hateblo.jp/entry/2016/10/06/222245

### 人工ニューロン

こうしたニューロンの仕組みを数理モデルにした人工ニューロン（ユニットと呼ばれる）を用いてネットワークを作ったものが**ニューラルネットワーク**（neural network）である。以下の図(a)は1つのユニットを，図(b)はニューラルネットワークの一例を示している。

![](13_NeuralNetwork.assets/1553736366556.png)

ユニットにはさまざまな入力$x_i$が異なる結合の強さ$w_i$で入ってきており，その重み付き和
$$
u = \sum_{i = 1}^p x_i w_i
$$
が総入力となる。$u$は**活性**（activation）とも呼ばれる。活性に**バイアス**（bias）と呼ばれる定数項$b$を加えて**活性化関数**（activation function）を通したものがこのユニットからの出力$z$となる。
$$
z = f(u + b) = f\left(\sum_{i = 1}^p x_i w_i + b \right)
$$
生体ニューロンにおいて総入力がある閾値を超えたときに信号を出力することを再現するのが活性化関数であり，**階段関数**（step function）
$$
\theta(x) = 
\begin{cases}
1 & (x \geq b)\\
0 & (x < b)
\end{cases}
$$
や**シグモイド関数**（sigmoid function）
$$
\sigma(x) = \frac{1}{1 + \exp(-x)}
$$
を使用する。



```{r, fig.height=4, fig.width=8, echo=F}
library(gridExtra)
library(tidyverse)

# 階段関数
step = function(x, b) ifelse(x >= -b, 1, 0)
# シグモイド関数
sigmoid = function(x) 1 / (1 + exp(-x))


# データ生成
x = -100:100 * 0.1

# plot
g1 <- ggplot(tibble(x, y = step(x, 0)),
             aes(x = x, y = y))+
  geom_line(color = "dodgerblue")+
  labs(y = expression(y), x = expression(x), title = "階段関数")

g2 <- ggplot(tibble(x, y = sigmoid(x)),
             aes(x = x, y = y))+
  geom_line(color = "dodgerblue")+
  labs(y = expression(y), x = expression(x), title = "シグモイド関数")

grid.arrange(g1, g2, ncol = 2)
```


重み$w_i$とバイアス$b$はデータから学習させる。その際には，まず重みの初期値としてランダムに設定し，その後は誤差を下げる方向に重みの更新を繰り返していくように推定していく。


## Rで実践

### データの準備

`{carData}`パッケージに含まれるタイタニック号の乗客データ`TitanicSurvival`を使う。

```{r}
# 準備
library(tidyverse)
set.seed(666)

# データ読み込み
library(carData)
data("TitanicSurvival")
head(TitanicSurvival)
```

このデータセットは1912年のタイタニック号の沈没事故の乗客の生死に関するデータで，次の変数が含まれている

- `survived`：生存したかどうか
- `sex`：性別
- `age`：年齢（1歳に満たない幼児は小数）。263の欠損値を含む。
- `passengerClass`：船室の等級

このデータセットには欠損値が含まれているため，まず欠損値を除去する

```{r}
# NA（欠損値）を含む行を削除
tita <- na.omit(TitanicSurvival)
```

そしてデータを学習用・テスト用に分割する。

```{r}
# ID列を追加
df = tita %>% rownames_to_column("ID")

# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)

# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")

# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
```

### ニューラルネット

中間層が1層のニューラルネットワークは`{nnet}`パッケージで実行することができる。

引数に指定している`size`は中間層のユニット数，`decay`は重みを更新するときの減衰率である。

<!-- nnetではBroyden, Fletcher, Goldfarb and Shanno(1970)の準ニュートン法を用いてパラメータを推定する。 -->

```{r}
# 単一中間層
library(nnet)
titanic_nnet <- nnet(survived ~ . , data = train,
                 size = 2, decay = 0.1)
```

訓練データでの正解率は以下のように計算できる。
```{r}
# 予測
y_pred_train = predict(titanic_nnet, train, type = "class")

# 混同行列
table(train$survived, y_pred_train)

# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_train, y_true = train$survived)
```

テストデータでの正解率は以下のように計算できる。
```{r}
# 予測
y_pred_test = predict(titanic_nnet, test, type = "class")

# 混同行列
table(test$survived, y_pred_test)

# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_test, y_true = test$survived)
```



# ディープラーニング

## 登場の経緯

ディープラーニングは多層ニューラルネットワークを用いた手法である。中間層を多層化するとニューラルネットの表現力が向上し，複雑な識別境界を描くことができる。

しかし，1990年代~2000年代には

1. **勾配消失**：活性化関数にシグモイド関数を用いて多層にすると，浅い層のパラメータが出力に影響を与えなくなって更新されなくなり学習が困難になる
2. **過学習**：表現力が向上する一方で過学習しやすくなる
3. **計算コスト**：並列計算が必要で，従来のCPUはこのタスクに向かない

といった問題が解決できなかった。

その後研究が進み，これらの問題は次のように解決できるようになった。

1. 勾配消失 → **ReLU**（rectified linear unit，活性化関数に正規化線形関数を使ったユニット）を使用する
   - $f(x) = \max(x, 0)$という活性化関数を使用する
2. 過学習 → **Dropout**：中間層のニューロンを学習のたびにいくつかランダムに無効化しつつ学習する
3. 計算コスト → **General Purpose GPU（GPGPU）**：並列演算用の処理装置の登場

そして2012年に画像認識のコンテストILSVRC2012においてディープラーニングを使用したチームが他のチームを圧倒する成績を出したことでその性能が注目されるようになり，ディープラーニングのブームが始まった。現在もディープラーニング（AI，人工知能）ブームは続いている。

## Rで実践

### データの用意

```{r}
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)

# 乱数の種を固定
set.seed(0)
```

[MNIST database](http://yann.lecun.com/exdb/mnist/)の手書き文字の画像データを使う。

```{r}
# MNISTデータのダウンロード
if (!dir.exists('data')) { # もしdataディレクトリがないなら作成
    dir.create('data')
}
if (!file.exists('data/train.csv')) { # もしdataディレクトリにtrain.csvがないならダウンロード
    download.file(url='https://raw.githubusercontent.com/wehrley/Kaggle-Digit-Recognizer/master/train.csv',
                  destfile='data/train.csv')
}
```

ダウンロードしたら、まずずデータを読み込む。

```{r}
# データの読み込み ---------------
mnist <- read.csv('data/train.csv')
```

どんなデータか、みてみる。

```{r}
head(mnist[1,]) # 1行目
```

`label`は数字（目的変数）、`pixel0`~`pixel783`は、28 x 28ピクセルの手書き文字の画像データ（特徴量＝説明変数）。

次に、訓練データとテストデータに分ける。

```{r}
# testとtrainに分割 --------------
# ID列を追加
df = mnist %>% rownames_to_column("ID")

# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)

# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")

# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
```

次いで、行列型(`data.matrix`)に変換し、目的変数と特徴量に分け、特徴量を正規化する。

```{r}
# その他の前処理 ----------------
# 行列型に変換
train <- data.matrix(train)
test <- data.matrix(test)

# 教師データ（目的変数）= 1列目
train_y <- train[,1]
test_y <- test[,1]

# 特徴量（説明変数）= 残りの784列
train_features <- train[,-1]
test_features <- test[,-1]

# 0から1の値になるよう正規化し、(数値が0~255なので、255で割る)、行と列をt()で入れ替える
train_X <- t(train_features/255)
test_X <- t(test_features/255)
```

このデータは次のような28 x 28ピクセルの手書き数字データになっている。

```{r}
# 扱っているデータを覗く
## 1つ目
i = 1 # 任意の番号のデータ
pixels = matrix(train_X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((0:255)/255), 
      xlab="", ylab="", main=paste("Label for this image:", train_y[i]))

## 2つ目
i = 3 # 任意の番号のデータ
pixels = matrix(train_X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((0:255)/255), 
      xlab="", ylab="", main=paste("Label for this image:", train_y[i]))
```

目的変数（教師データ）は0~9のラベルで，それぞれ3000レコード程度ある

```{r}
# 教師データのラベル
table(train_y)
```

### DNN (Deep Nueral Net)

ディープラーニングを行うには`{mxnet}`パッケージを使用する。インストール方法が少し面倒だが次のように書いてインストールすればよい
（参考：[mxnetの公式サイト](http://mxnet.incubator.apache.org/versions/master/install/index.html)）。

```{r, eval=F}
# mxnetのインストール
cran <- getOption("repos")
cran["dmlc"] <- "https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/"
options(repos = cran)
install.packages("mxnet")
```


まずネットワーク構造を決める。以下では多層ニューラルネットワークを使ってみる。

```{r}
library(mxnet)
# ネットワーク構造の宣言： Deep Neural Network

data <- mx.symbol.Variable("data")
# 第1中間層
fc1 <- mx.symbol.FullyConnected(data, name="fc1", num_hidden=128) # ユニット数128
act1 <- mx.symbol.Activation(fc1, name="relu1", act_type="relu")  # ReLUの活性化関数
# 第2中間層
fc2 <- mx.symbol.FullyConnected(act1, name="fc2", num_hidden=64) # ユニット数64
act2 <- mx.symbol.Activation(fc2, name="relu2", act_type="relu") # ReLUの活性化関数
# 第3中間層
fc3 <- mx.symbol.FullyConnected(act2, name="fc3", num_hidden=10) # ユニット数10
softmax <- mx.symbol.SoftmaxOutput(fc3, name="sm") # softmax活性化関数（シグモイド関数の多値バージョン）
```


```{r}
# 計算に使うデバイスを宣言
devices <- mx.cpu()
```

```{r}
# ニューラルネットを訓練
mx.set.seed(0)
dnn_model <- mx.model.FeedForward.create(softmax, X=train_X, y=train_y,
                                     ctx=devices, num.round=10, array.batch.size=100,
                                     learning.rate=0.07, momentum=0.9,  eval.metric=mx.metric.accuracy,
                                     initializer=mx.init.uniform(0.07),
                                        epoch.end.callback=mx.callback.log.train.metric(100))
```

訓練データでの予測。

```{r}
# 予測
pred_train <- predict(dnn_model, train_X)
predicted_labels_train <- max.col(t(pred_train)) - 1

# 混同行列
table(train_y, predicted_labels_train)
```


テストデータでの予測。

```{r}
# 予測
pred_test <- predict(dnn_model, test_X)
predicted_labels_test <- max.col(t(pred_test)) - 1

# 混同行列
table(test_y, predicted_labels_test)
```










