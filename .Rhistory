TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university_temp = case_when(exemption == 1 ~ 1,
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
df_temp3 <- df_temp2 %>% mutate(binom_harf = rbinom(10000, 1, 0.5))
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ 1,
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ binom_harf,
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(binom_harf),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ rbinom(10000, 1, 0.5),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(rbinom(10000, 1, 0.5)),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(rbinom(n_exemption, 1, 0.5)),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(rbinom(1, 1, 0.5)),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(rbinom(1000, 1, 0.5)),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp3 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(rbinom(1, 1, 0.5)),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp2 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(rbinom(1, 1, 0.5)),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるように変更
#　mutate()の中でcase_when()を使い、excemption = 1 あるいは university = 1のときはuniversity=1とし、それ以外はuniversity=0とする
df <- df_temp2 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(rbinom(10000, 1, 0.5)),
university == 1 ~ 1,
TRUE ~ 0)) # それ以外は0
# exemption=1のときに50%の確率でuniversity=1となるようにする
# mutate()の中でcase_when()を使う
# case_when()での場合分けでの数字生成がnumericで行われるように、as.numeric()の中でrbinomを使って0,1の乱数を生成
df <- df_temp2 %>%
mutate(university = case_when(exemption == 1 ~ as.numeric(rbinom(10000, 1, 0.5)), # exepmmption=1なら50%の確率で1. n=10000にする必要あり
university == 1 ~ 1, #すでにuniversity=1なら1
TRUE ~ 0)) # それ以外は0
# check sample size of university = 1
summury(df)
# check sample size of university = 1
sum(df)
# check sample size of university = 1
summury(df)
# check sample size of university = 1
summary(df)
# check sample size of university = 1
describe(df)
# check sample size of university = 1
library(Hmisc)
install.packages("Hmisc")
library
# check sample size of university = 1
library(Hmisc)
describe(df)
# 所得
df["income"] = 200 + 10*df["ability"] + 500*df["university"] + rnorm(n, mean = 0, sd = 50)
install.packages("glmnet")
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(tidyverse)
library(MLmetrics)
set.seed(0)
# データ生成 ----
n = 20
# x = rnorm(n, sd = 0.5)
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
df <- df %>% select(-ID)
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 3
ggplot(train, aes(x, y))+
geom_point()
# Chunk 4
# plot
p = 1
p1 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 1))+
labs(title = str_c(p,"次多項式回帰"))
p = 2
p2 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2))+
labs(title = str_c(p,"次多項式回帰"))
p = 3
p3 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 3))+
labs(title = str_c(p,"次多項式回帰"))
p = 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 5))+
labs(title = str_c(p,"次多項式回帰"))
p = 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 7))+
labs(title = str_c(p,"次多項式回帰"))
p = 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 9))+
labs(title = str_c(p,"次多項式回帰"))
library(gridExtra)
grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- data_frame(p, train_error, test_error)　# p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, data_frame(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# trainに対する予測精度
y_train_pred <- predict(ridge_cv, X_train, s = "lambda.min") # 予測
# ラムダを変えながら交差検証で誤差を評価
lasso_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
alpha = 1) # alpha = 1でlasso
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(lasso_cv, s = "lambda.min") %>% round(3)
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(tidyverse)
library(MLmetrics)
set.seed(0)
# データ生成 ----
n = 20
# x = rnorm(n, sd = 0.5)
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
df <- df %>% select(-ID)
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 3
ggplot(train, aes(x, y))+
geom_point()
# Chunk 4
# plot
p = 1
p1 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 1))+
labs(title = str_c(p,"次多項式回帰"))
p = 2
p2 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2))+
labs(title = str_c(p,"次多項式回帰"))
p = 3
p3 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 3))+
labs(title = str_c(p,"次多項式回帰"))
p = 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 5))+
labs(title = str_c(p,"次多項式回帰"))
p = 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 7))+
labs(title = str_c(p,"次多項式回帰"))
p = 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 9))+
labs(title = str_c(p,"次多項式回帰"))
library(gridExtra)
grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
# Chunk 5
# 次数を変えながら学習・予測
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- data_frame(p, train_error, test_error) # p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, data_frame(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# ggplotで多項式回帰と誤差(RMSE)の関係を描画
ggplot(result %>% gather(key, error, -p),
aes(x = p, y = error, color = key))+
geom_point()+
geom_line(size = 1)+
scale_x_continuous(breaks = c(1:9))+
labs(color = "", x = "多項式回帰の次数", y = "誤差（RMSE)",
title = "モデルの複雑さと過学習")
# Chunk 6
# パラメータ空間
# df_plot = expand.grid(beta_1 = -50:50 * 1/50,
#                      beta_2 = -50:50 * 1/50) %>%
#   mutate(beta_L2 = beta_1^2 + beta_2^2) %>%
#   mutate(isR = 1*(beta_L2 <= 0.5))
#
# ggplot(df_plot, aes(x = beta_1, y = beta_2, color = isR))+
#   geom_point()+
#   geom_hline(yintercept = 0)+
#   geom_vline(xintercept = 0)
# Chunk 7
# data
x1 <- rnorm(n = 100, mean = 5, sd = 3)
e <- rnorm(n = 100, mean = 5, sd = 6)
X <- data_frame(x1,
x2 = 2*x1 + e)
# plot
g1 <- ggplot(X, aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "元のデータ")
g2 <- ggplot(scale(X) %>% data.frame(), aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "標準化後のデータ")
grid.arrange(g1,g2, ncol = 2)
# Chunk 8
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)
# 乱数の種を固定
set.seed(0)
# データ生成 ----
n = 20
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
# xの2乗～9乗の変数を作成
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 9
# glmnetは説明変数Xと目的変数yを別々に入れる必要があるので分ける
# train
X_train = train %>% select(-y) %>% as.matrix() # as.matrix()でmatrix型のデータにする
y_train = train$y
# test
X_test = test %>% select(-y) %>% as.matrix()
y_test = test$y
# Chunk 11
# 9次多項式でリッジ回帰
library(glmnet)
reg_ridge <- glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 0.1, # λ：任意の値
alpha = 0) # alpha = 0でリッジ回帰
# trainに対する予測精度
y_train_pred <- predict(reg_ridge, X_train) # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(reg_ridge, X_test) # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
# 結果を表示(str_c:複数のstringsを1つにまとめる)
str_c("test_error: ", round(test_error, 3)) %>% print()
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
# ラムダと交差検証誤差のグラフ
plot(ridge_cv)
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(tidyverse)
library(MLmetrics)
set.seed(0)
# データ生成 ----
n = 20
# x = rnorm(n, sd = 0.5)
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
df <- df %>% select(-ID)
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 3
ggplot(train, aes(x, y))+
geom_point()
# Chunk 4
# plot
p = 1
p1 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 1))+
labs(title = str_c(p,"次多項式回帰"))
p = 2
p2 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2))+
labs(title = str_c(p,"次多項式回帰"))
p = 3
p3 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 3))+
labs(title = str_c(p,"次多項式回帰"))
p = 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 5))+
labs(title = str_c(p,"次多項式回帰"))
p = 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 7))+
labs(title = str_c(p,"次多項式回帰"))
p = 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 9))+
labs(title = str_c(p,"次多項式回帰"))
library(gridExtra)
grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
# Chunk 5
# 次数を変えながら学習・予測
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- data_frame(p, train_error, test_error) # p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, data_frame(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# ggplotで多項式回帰と誤差(RMSE)の関係を描画
ggplot(result %>% gather(key, error, -p),
aes(x = p, y = error, color = key))+
geom_point()+
geom_line(size = 1)+
scale_x_continuous(breaks = c(1:9))+
labs(color = "", x = "多項式回帰の次数", y = "誤差（RMSE)",
title = "モデルの複雑さと過学習")
# Chunk 6
# パラメータ空間
# df_plot = expand.grid(beta_1 = -50:50 * 1/50,
#                      beta_2 = -50:50 * 1/50) %>%
#   mutate(beta_L2 = beta_1^2 + beta_2^2) %>%
#   mutate(isR = 1*(beta_L2 <= 0.5))
#
# ggplot(df_plot, aes(x = beta_1, y = beta_2, color = isR))+
#   geom_point()+
#   geom_hline(yintercept = 0)+
#   geom_vline(xintercept = 0)
# Chunk 7
# data
x1 <- rnorm(n = 100, mean = 5, sd = 3)
e <- rnorm(n = 100, mean = 5, sd = 6)
X <- data_frame(x1,
x2 = 2*x1 + e)
# plot
g1 <- ggplot(X, aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "元のデータ")
g2 <- ggplot(scale(X) %>% data.frame(), aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "標準化後のデータ")
grid.arrange(g1,g2, ncol = 2)
# Chunk 8
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)
# 乱数の種を固定
set.seed(0)
# データ生成 ----
n = 20
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
# xの2乗～9乗の変数を作成
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 9
# glmnetは説明変数Xと目的変数yを別々に入れる必要があるので分ける
# train
X_train = train %>% select(-y) %>% as.matrix() # as.matrix()でmatrix型のデータにする
y_train = train$y
# test
X_test = test %>% select(-y) %>% as.matrix()
y_test = test$y
# Chunk 11
# 9次多項式でリッジ回帰
library(glmnet)
reg_ridge <- glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 0.1, # λ：任意の値
alpha = 0) # alpha = 0でリッジ回帰
# trainに対する予測精度
y_train_pred <- predict(reg_ridge, X_train) # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(reg_ridge, X_test) # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
# 結果を表示(str_c:複数のstringsを1つにまとめる)
str_c("test_error: ", round(test_error, 3)) %>% print()
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
)
# ラムダと交差検証誤差のグラフ
plot(ridge_cv)
