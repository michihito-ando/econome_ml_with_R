plot(titanic_nnet)
install.packages("NeuralNetTools")
library(NeuralNetTools)
plotnet(nn1)
plotnet(titanic_nnet)
plotnet(mnist_dl)
plotnet(model)
remove.packages("NeuralNetTools")
if(!require(NeuralNetTools)) {install.packages("NeuralNetTools")}
library(NeuralNetTools)
plotnet(titanic_nnet)
if(!require(NeuralNetTools)) {install.packages("NeuralNetTools")}
library(NeuralNetTools)
plotnet(titanic_nnet)
y_true = as.vector(test[,1])
y_pred = as.vector(pred[,1])
# 混同行列
table(y_true, y_pred)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred, y_true = y_true)
# 学習
mnist_dl = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train,      # 学習用データを指定
activation = c("RectifierWithDropout"), # 活性化関数を指定
hidden = c(128, 64, 16),     # 中間層（隠れ層）のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させて重みを更新していくか。
hidden_dropout_ratios = c(0.5, 0.5, 0.5),  # 各中間層でdropoutする割合
sparse = TRUE,      # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ使用量を抑える。
seed = 0
)
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(gridExtra)
library(tidyverse)
# 階段関数
step = function(x, b) ifelse(x >= -b, 1, 0)
# シグモイド関数
sigmoid = function(x) 1 / (1 + exp(-x))
# データ生成
x = -100:100 * 0.1
# plot
g1 <- ggplot(tibble(x, y = step(x, 0)),
aes(x = x, y = y))+
geom_line(color = "dodgerblue")+
labs(y = expression(y), x = expression(x), title = "階段関数")
g2 <- ggplot(tibble(x, y = sigmoid(x)),
aes(x = x, y = y))+
geom_line(color = "dodgerblue")+
labs(y = expression(y), x = expression(x), title = "シグモイド関数")
grid.arrange(g1, g2, ncol = 2)
# Chunk 3
# 準備
library(tidyverse)
set.seed(666)
# データ読み込み
library(carData)
data("TitanicSurvival")
head(TitanicSurvival)
# Chunk 4
# NA（欠損値）を含む行を削除
tita <- na.omit(TitanicSurvival)
# Chunk 5
# ID列を追加
df = tita %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 6
# 単一中間層
library(nnet)
titanic_nnet <- nnet(survived ~ . , data = train,
size = 2, decay = 0.1)
# Chunk 7
if(!require(NeuralNetTools)) {install.packages("NeuralNetTools")} # インストールされていなければインストールする
library(NeuralNetTools)
plotnet(titanic_nnet)
# Chunk 8
# 予測
y_pred_train = predict(titanic_nnet, train, type = "class")
# 混同行列
table(train$survived, y_pred_train)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_train, y_true = train$survived)
# Chunk 9
# 予測
y_pred_test = predict(titanic_nnet, test, type = "class")
# 混同行列
table(test$survived, y_pred_test)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_test, y_true = test$survived)
# Chunk 10
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)
# 乱数の種を固定
set.seed(0)
# Chunk 11
# MNISTデータのダウンロード
if (!dir.exists('data')) { # もしdataディレクトリがないなら作成
dir.create('data')
}
if (!file.exists('data/train.csv')) { # もしdataディレクトリにtrain.csvがないならダウンロード
download.file(url='https://raw.githubusercontent.com/wehrley/Kaggle-Digit-Recognizer/master/train.csv',
destfile='data/train.csv')
}
# Chunk 12
# データの読み込み
mnist <- read.csv('data/train.csv')
# Chunk 13
# 1レコード目の1~10列
mnist[1, 1:10]
# 1レコード目の210~220列
mnist[1, 210:220]
# 1レコード目の最後の10列
k = ncol(mnist)
mnist[1, (k-10):k]
# Chunk 14
# 教師データ（目的変数）= 1列目
y <- mnist[,1]
# 特徴量（説明変数）= 残りの784列
X <- mnist[,-1]
# 0から1の値になるよう正規化し、(数値が0~255なので、255で割る)、行と列をt()で入れ替える
X <- t(X/255)
# 扱っているデータを覗く
i = 10 # 10レコード目
pixels = matrix(X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((255:0)/255),
xlab="", ylab="", main=paste("Label for this image:", y[i]))
i = 1000 # 1000レコード目
pixels = matrix(X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((255:0)/255),
xlab="", ylab="", main=paste("Label for this image:", y[i]))
# Chunk 15
# testとtrainに分割
# ID列を追加
df = mnist %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 16
# 教師データのラベル
table(train[, 1])
# Chunk 17
# ライブラリの用意
if(!require(h2o)) {install.packages("h2o")}
library(h2o)
# 初期化
h2o.init()
# データをh2o用のデータ型に変換
train <- as.h2o(train)
test <- as.h2o(test)
# 1列目（目的変数）をfactor型に変換
train[,1] = h2o::as.factor(train[,1])
test[,1] = h2o::as.factor(test[,1])
# Chunk 18
# trainデータを学習用のものと検証用のものに分ける
splits <- h2o.splitFrame(train, ratios = 0.8, seed = 0)
# Chunk 19
# 学習
mnist_dl = h2o.deeplearning(
x = 2:ncol(train), # 特徴量の列番号を指定
y = 1,             # 目的変数の列番号を指定
training_frame = splits[[1]],   # 訓練データを指定
validation_frame = splits[[2]], # 検証データを指定（学習には使わず、精度を測るためだけに使う）
activation = c("RectifierWithDropout"), # 活性化関数を指定
hidden = c(64, 32, 16), # 中間層（隠れ層）のサイズ
epochs = 20,       # エポック数。学習データ何回分の学習を反復させて重みを更新していくか。
hidden_dropout_ratios = c(0.5, 0.5, 0.5), # 各中間層においてdropoutするユニットの割合
sparse = TRUE,     # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ使用量を抑える。
seed = 0
)
# Chunk 20
mnist_dl
# エポック数ごとの訓練データに対する予測誤差の推移
plot(mnist_dl)
mnist_dl
install.packages("DiagrammeR")
install.packages("DiagrammeR")
library(DiagrammeR)
install.packages("DiagrammeR")
library(glue)
install.packages("glue")
install.packages("DiagrammeR")
library(glue)
install.packages(c("backports", "bayestestR", "BH", "blob", "broom", "car", "carData", "caret", "class", "cli", "conquer", "corrplot", "crayon", "crosstalk", "data.table", "datawizard", "DBI", "digest", "DT", "dtplyr", "estimatr", "fansi", "flextable", "forecast", "foreign", "fs", "future", "generics", "ggsignif", "glmnet", "glue", "googlesheets4", "haven", "hms", "htmltools", "htmlwidgets", "insight", "ipred", "isoband", "jsonlite", "knitr", "later", "lattice", "lava", "lifecycle", "lmtest", "lubridate", "maptools", "MASS", "Matrix", "matrixStats", "mgcv", "mime", "modelsummary", "nlme", "nloptr", "nnet", "officer", "openssl", "openxlsx", "padr", "parallelly", "parameters", "performance", "pillar", "plotly", "pROC", "psych", "Quandl", "R6", "Rblpapi", "Rcpp", "RcppArmadillo", "readr", "recipes", "remotes", "repr", "reprex", "rio", "rJava", "rlang", "rmarkdown", "rsample", "Rttf2pt1", "rvest", "sp", "spatial", "stringi", "survival", "systemfonts", "tibble", "tidyr", "timetk", "tinytex", "tseries", "TTR", "utf8", "uuid", "withr", "xfun", "xml2"))
pacman::p_load(DiagrammeR)
# 事前準備 --------------------
# パッケージの読み込み
pacman(tidyverse, estimatr, plm, stargazer,texreg)
# 事前準備 --------------------
# パッケージの読み込み
library(pacman)
pacman(tidyverse, estimatr, plm, stargazer,texreg)
pacman::p_load(tidyverse, estimatr, plm, stargazer,texreg)
pacman::p_load(tidyverse, estimatr, plm, stargazer,texreg)
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F,
fig.height=3, fig.width=4)
# Chunk 3
# パッケージの読み込み
library(AER)
# データの読み込み
data("CPS1985")
# Chunk 4
head(CPS1985)
pacman::p_load(readxl, tidyverse, skimr, AER)
pacman::p_load(readxl, tidyverse, skimr, AER)
# データの読み込み
data("CPS1985")
head(CPS1985)
summary(CPS1985)
skimr::skim(CPS1985)
# データの読み込み
data("CPS1985")
head(CPS1985)
#textでの表示
stargazer(CPS1985, type = "text")
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F,
fig.height=3, fig.width=4)
# Chunk 2
pacman::p_load(readxl, tidyverse, skimr, AER, stargazer)
# Chunk 3
# データの読み込み
data("CPS1985")
head(CPS1985)
# Chunk 4
summary(CPS1985)
skimr::skim(CPS1985)
#textでの表示
stargazer(CPS1985, type = "text")
#htmlで表示し、summary_CPS1985.htmlという名前での保存
stargazer(CPS1985, type = "html", out = "summary_CPS1985.html")
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F,
fig.height=3, fig.width=4)
# Chunk 2
pacman::p_load(readxl, tidyverse, skimr, AER, stargazer, gridExtra, estimatr)
# Chunk 3
# データの読み込み
data("CPS1985")
head(CPS1985)
# Chunk 4
summary(CPS1985)
skimr::skim(CPS1985)
# Chunk 5
#textでの表示
stargazer(CPS1985, type = "text")
# Chunk 6
#htmlで表示し、summary_CPS1985.htmlという名前での保存
stargazer(CPS1985, type = "html", out = "summary_CPS1985.html")
# Chunk 7
# Regression
reg <- lm(formula = wage ~ education,
data = CPS1985) # 教育年数が賃金を決めるモデル
reg
# Chunk 8
summary(reg)
# Chunk 9
stargazer(reg, type = "text")
# Chunk 11
g1 <- ggplot(CPS1985, aes(x = wage))+
geom_histogram(bins = 10)
g2 <- ggplot(CPS1985, aes(x = log(wage)))+
geom_histogram(bins = 10)
grid.arrange(g1, g2) #gridextraパッケージによるグラフの結合。他にもpatchworkパッケージなどがある。
# Chunk 12
# g1 <- ggplot(CPS1985, aes(x = education, y = wage))+
#   geom_point(alpha = 0.5)+
#   geom_smooth(method = "loess", se = F)
# g2 <- ggplot(CPS1985, aes(x = education, y = log(wage)))+
#   geom_point(alpha = 0.5)+
#   geom_smooth(method = "loess", se = F)
# Chunk 13
reg_logwage <- lm(formula = log(wage) ~ education,
data = CPS1985)
# Chunk 14
stargazer(reg, reg_logwage, type = "text")
# Chunk 15
reg_multiple <- lm(formula = log(wage) ~ education + experience, data = CPS1985)
# Chunk 16
stargazer(reg_logwage, reg_multiple, type = "text")
# mutate関数で新しい変数を追加したデータをdfオブジェクトに代入
df = CPS1985 %>% dplyr::mutate(experience2 = experience^2)
reg_mincer <- lm(formula = log(wage) ~ education + experience + experience2, data = df)
reg_mincer <- lm(formula = log(wage) ~
education + experience + experience2,
data = df)
stargazer(reg_logwage, reg_multiple, reg_mincer,
type = "html",
out = "04reg_table.html")
# パッケージの読み込み
library("estimatr")
# ロバスト標準誤差を用いたOLS推定
reg_logwage_r <-
lm_robust(formula = log(wage) ~ education,
data = CPS1985, se_type = "HC1")
reg_multiple_r <-
lm_robust(formula = log(wage) ~ education + experience,
data = CPS1985, se_type = "HC1")
reg_mincer_r <-
lm_robust(formula = log(wage) ~ education + experience + experience2,
data = df, se_type = "HC1")
summary(reg_logwage_r)
summary(reg_multiple_r)
summary(reg_mincer_r)
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F,
fig.height=3, fig.width=4)
# Chunk 2
pacman::p_load(readxl, tidyverse, skimr, AER, stargazer, gridExtra, estimatr, texreg)
# Chunk 4
# データの読み込み
data("CPS1985")
head(CPS1985)
# Chunk 5
summary(CPS1985)
skimr::skim(CPS1985)
# Chunk 6
#textでの表示
stargazer(CPS1985, type = "text")
# Chunk 7
#htmlで表示し、summary_CPS1985.htmlという名前での保存
stargazer(CPS1985, type = "html", out = "summary_CPS1985.html")
# Chunk 8
# Regression
reg <- lm(formula = wage ~ education,
data = CPS1985) # 教育年数が賃金を決めるモデル
reg
# Chunk 9
summary(reg)
# Chunk 10
stargazer(reg, type = "text")
# Chunk 12
g1 <- ggplot(CPS1985, aes(x = wage))+
geom_histogram(bins = 10)
g2 <- ggplot(CPS1985, aes(x = log(wage)))+
geom_histogram(bins = 10)
grid.arrange(g1, g2) #gridextraパッケージによるグラフの結合。他にもpatchworkパッケージなどがある。
# Chunk 13
# g1 <- ggplot(CPS1985, aes(x = education, y = wage))+
#   geom_point(alpha = 0.5)+
#   geom_smooth(method = "loess", se = F)
# g2 <- ggplot(CPS1985, aes(x = education, y = log(wage)))+
#   geom_point(alpha = 0.5)+
#   geom_smooth(method = "loess", se = F)
# Chunk 14
reg_logwage <- lm(formula = log(wage) ~ education,
data = CPS1985)
# Chunk 15
stargazer(reg, reg_logwage, type = "text")
# Chunk 16
reg_multiple <- lm(formula = log(wage) ~ education + experience, data = CPS1985)
# Chunk 17
stargazer(reg_logwage, reg_multiple, type = "text")
# Chunk 18
# mutate関数で新しい変数を追加したデータをdfオブジェクトに代入
df = CPS1985 %>% dplyr::mutate(experience2 = experience^2)
# Chunk 19
reg_mincer <- lm(formula = log(wage) ~
education + experience + experience2,
data = df)
# Chunk 20
stargazer(reg_logwage, reg_multiple, reg_mincer,
type = "text")
# Chunk 21
stargazer(reg_logwage, reg_multiple, reg_mincer,
type = "html",
out = "04reg_table.html")
# Chunk 23
# パッケージの読み込み
library("estimatr")
# ロバスト標準誤差を用いたOLS推定
reg_logwage_r <-
lm_robust(formula = log(wage) ~ education,
data = CPS1985, se_type = "HC1")
reg_multiple_r <-
lm_robust(formula = log(wage) ~ education + experience,
data = CPS1985, se_type = "HC1")
reg_mincer_r <-
lm_robust(formula = log(wage) ~ education + experience + experience2,
data = df, se_type = "HC1")
# Chunk 24
summary(reg_logwage_r)
summary(reg_multiple_r)
summary(reg_mincer_r)
#結果表の出力
screenreg(list(reg_logwage_r, reg_multiple_r, reg_mincer_r),
include.ci = FALSE, digits = 3)
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
pacman::p_load(tidyverse, plotly, DiagrammeR)
# Chunk 3
grViz("digraph dot{
graph[rankdir = LR]
node[shape = circle, fontname = 'Yu Gothic']
edge[fontname = 'Yu Gothic', fontsize = 10]
能力A -> 所得Y [label='10']
能力A -> 学歴X [label='A≧80からランダム抽出']
学歴X -> 所得Y [label='500']
{rank = same; 能力A; 学歴X}
}")
# grViz("digraph dot{
# graph[rankdir = LR]
#
# node[shape = circle, fontname = 'Yu Gothic']
#
# edge[]
# 能力A -> {所得Y; 学歴X; テストの点}
# テストの点 -> 学歴X -> 所得Y
#       }")
# 乱数の種を固定　=>　毎回同じように乱数を発生させるようにする。0を他の数値に変えると異なる乱数となる。
set.seed(0)
# n:サンプルサイズ
n <- 10000
# 能力は0から100まで均等に分布。#runifは一様分布を発生させる関数。標本規模n、最小値0、最大値100
ability <- runif(n, min = 0, max = 100)
df <- tibble(ID = 1:n, ability)
university_df <- df %>% filter(ability >= 80) %>%
sample_frac(0.5) # 大卒の人
university_df["university"] = 1
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
pacman::p_load(tidyverse, plotly, DiagrammeR)
# Chunk 3
grViz("digraph dot{
graph[rankdir = LR]
node[shape = circle, fontname = 'Yu Gothic']
edge[fontname = 'Yu Gothic', fontsize = 10]
能力A -> 所得Y [label='10']
能力A -> 学歴X [label='A≧80からランダム抽出']
学歴X -> 所得Y [label='500']
{rank = same; 能力A; 学歴X}
}")
# grViz("digraph dot{
# graph[rankdir = LR]
#
# node[shape = circle, fontname = 'Yu Gothic']
#
# edge[]
# 能力A -> {所得Y; 学歴X; テストの点}
# テストの点 -> 学歴X -> 所得Y
#       }")
# 乱数の種を固定　=>　毎回同じように乱数を発生させるようにする。0を他の数値に変えると異なる乱数となる。
set.seed(0)
# n:サンプルサイズ
n <- 10000
# 能力は0から100まで均等に分布。#runifは一様分布を発生させる関数。標本規模n、最小値0、最大値100
ability <- runif(n, min = 0, max = 100)
df <- tibble(ID = 1:n, ability)
university_df <- df %>% filter(ability >= 80) %>%
sample_frac(0.5) # 大卒の人
university_df["university"] = 1
View(university_df)
university_df <- university_df %>% dplyr::mutate(university == 1)
university_df <- university_df %>%
dplyr::mutate(university = 1)
# n:サンプルサイズ
n <- 10000
# 能力は0から100まで均等に分布。#runifは一様分布を発生させる関数。標本規模n、最小値0、最大値100
ability <- runif(n, min = 0, max = 100)
df <- tibble(ID = 1:n, ability)
university_df <- df %>% filter(ability >= 80) %>%
sample_frac(0.5) # 大卒の人
university_df <- university_df %>%
dplyr::mutate(university = 1)
# dfからdplyr::anti_join()を用いて"university_df"とマッチしなかった人を抽出する。
no_university_df <- anti_join(df, university_df,
by = c("ID","ability")) # 大卒ではない人
View(no_university_df)
# no_university_dfのデータフレームに、universityという変数を作成し、すべて0とする。
no_university_df <- no_university_df %>%
dplyr::mutate(university = 0)
# university_dfとno_university_dfを、dplyr::bind_rowsを用いて結合してあたらしいdfとし、ID順で並べる
df <- bind_rows(university_df, no_university_df) %>%  # 両者を結合
arrange(ID) # ID順に並べる
df <- df %>% mutate(income = 200 + 10*ability + 500*university + rnorm(n, mean = 0, sd = 50)) # 誤差項は平均=0、SD=50
# 最初の6行
head(df)
## 大卒か否かのラベルをデータフレームに加える
df <- df %>% mutate(edu_label =
case_when(university == 1 ~ "Grad.",
university == 0 ~ "Not grad."))
## 散布図を描く with 大卒ラベル
ggplot(df, aes(x = ability, y = income, color = edu_label)) +
geom_point(alpha = 0.5)+
labs(title = "Ability and Income")
# 塗り分けプロット
# plot
## 散布図と回帰直線を描く with 大卒ラベル
ggplot(df, aes(x = ability, y = income, color = edu_label, group = edu_label)) + # groupごと
geom_point(alpha = 0.5)+
geom_smooth(method = "lm", color = "black") +  # 回帰直線
labs(title = "Ability and Income")
ols_S <- lm(formula = income ~ university, data = df)
stargazer(ols_S, type = "text")
