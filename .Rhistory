y <- mnist[,1]
# 特徴量（説明変数）= 残りの784列
X <- mnist[,-1]
# 0から1の値になるよう正規化し、(数値が0~255なので、255で割る)、行と列をt()で入れ替える
X <- t(X/255)
# 扱っているデータを覗く
i = 10 # 10レコード目
pixels = matrix(X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((255:0)/255),
xlab="", ylab="", main=paste("Label for this image:", y[i]))
i = 1000 # 1000レコード目
pixels = matrix(X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((255:0)/255),
xlab="", ylab="", main=paste("Label for this image:", y[i]))
# testとtrainに分割
# ID列を追加
df = mnist %>% rownames_to_column("ID")
df
# testとtrainに分割
# ID列を追加
df = mnist %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
library(tidyverse)
# データの読み込み
mnist <- read.csv('data/train.csv')
# testとtrainに分割
# ID列を追加
df = mnist %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
rm(test, train, mnist)
test
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
train
# ライブラリの用意
if (!require(h2o)) {install.packages("h2o")}
library(tidyverse)
# データの読み込み
mnist <- read.csv('data/train.csv')
# testとtrainに分割 --------
# ID列を追加
df = mnist %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# ライブラリの用意
if (!require(h2o)) {install.packages("h2o")}
library(h2o)
h2o.init()
# データをh2o用のデータ型に変換
train <- as.h2o(train)
test <- as.h2o(test)
# 1列目（目的変数）をfactor型に変換
train[,1] = h2o::as.factor(train[,1])
test[,1] = h2o::as.factor(test[,1])
# ディープラーニングの学習
h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("RectifierWithDropout"),
hidden = c(10, 10), # 中間層のサイズ
rate = 0.01,        # 学習率
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
input_dropout_ratio = 0,
hidden_dropout_ratios = NULL,
sparse = TRUE       # 0が多いデータ（スパースデータ）のメモリ効率を高める
)
model = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("Rectifier"),
hidden = c(10, 10), # 中間層のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
hidden_dropout_ratios = 0.5,  # 中間層のdropout
sparse = TRUE       # 0が多いデータ（スパースデータ）のメモリ効率を高める
)
pred <- h2o.predict(model, X)
round(pred[,2:11], 3)
# ディープラーニングの学習
model = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("Rectifier"),
hidden = c(100, 100), # 中間層のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
hidden_dropout_ratios = 0.5,  # 中間層のdropout
sparse = TRUE       # 0が多いデータ（スパースデータ）のメモリ効率を高める
)
model
# ディープラーニングの学習
model = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("Rectifier"),
hidden = c(100, 100), # 中間層のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
hidden_dropout_ratios = 0.1,  # 中間層のdropout
sparse = TRUE       # 0が多いデータ（スパースデータ）のメモリ効率を高める
)
# ディープラーニングの学習
model = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("Rectifier"),
hidden = c(100, 100), # 中間層のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
# hidden_dropout_ratios = 0.1,  # 中間層のdropout
sparse = TRUE       # 0が多いデータ（スパースデータ）のメモリ効率を高める
)
pred <- h2o.predict(model, X)
round(pred[,2:11], 3)
pred <- h2o.predict(model, test)
round(pred[,2:11], 3)
pred
round(pred[,2:11], 3)
pred[, 1]
c(pred[, 1])
pred[, 1] - pred[, 1]
y_true = test[, 1]
y_pred = pred[, 1]
# 混同行列
table(y_true, y_pred)
test
data.table(test)
data.frame(test)
as.data.frame(pred)
as.data.frame(pred[,1])
as.vector(pred[,1])
y_true = as.vector(test[,1])
y_pred = as.vector(pred[,1])
# 混同行列
table(y_true, y_pred)
y_true = as.vector(test[,1])
y_pred = as.vector(pred[,1])
# 混同行列
table(y_pred, y_true)
y_true = as.vector(test[,1])
y_pred = as.vector(pred[,1])
# 混同行列
table(y_true, y_pred)
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(gridExtra)
library(tidyverse)
# 階段関数
step = function(x, b) ifelse(x >= -b, 1, 0)
# シグモイド関数
sigmoid = function(x) 1 / (1 + exp(-x))
# データ生成
x = -100:100 * 0.1
# plot
g1 <- ggplot(tibble(x, y = step(x, 0)),
aes(x = x, y = y))+
geom_line(color = "dodgerblue")+
labs(y = expression(y), x = expression(x), title = "階段関数")
g2 <- ggplot(tibble(x, y = sigmoid(x)),
aes(x = x, y = y))+
geom_line(color = "dodgerblue")+
labs(y = expression(y), x = expression(x), title = "シグモイド関数")
grid.arrange(g1, g2, ncol = 2)
# Chunk 3
# 準備
library(tidyverse)
set.seed(666)
# データ読み込み
library(carData)
data("TitanicSurvival")
head(TitanicSurvival)
# Chunk 4
# NA（欠損値）を含む行を削除
tita <- na.omit(TitanicSurvival)
# Chunk 5
# ID列を追加
df = tita %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 6
# 単一中間層
library(nnet)
titanic_nnet <- nnet(survived ~ . , data = train,
size = 2, decay = 0.1)
# Chunk 7
# 予測
y_pred_train = predict(titanic_nnet, train, type = "class")
# 混同行列
table(train$survived, y_pred_train)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_train, y_true = train$survived)
titanic_nnet
plot(titanic_nnet)
0.1 * 10
c(0.1) * 10
c(0.1) * 10
repeat()
repeat(1, 1, 1)
repeat(1, 1)
rep(0, 10)
rep(0.1, 10)
# 学習
model = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("Rectifier"),
hidden = c(10, 10), # 中間層のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
# hidden_dropout_ratios = 0.1,  # 中間層のdropout
hidden_dropout_ratios = rep(0.1, 10*10),
sparse = TRUE,      # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ効率を高める
seed = 0
)
library(tidyverse)
# データの読み込み
mnist <- read.csv('data/train.csv')
dim(mnist)
# testとtrainに分割 --------
# ID列を追加
df = mnist %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# ライブラリの用意
if (!require(h2o)) {install.packages("h2o")}
library(h2o)
h2o.init()
# データをh2o用のデータ型に変換
train <- as.h2o(train)
test <- as.h2o(test)
# 1列目（目的変数）をfactor型に変換
train[,1] = h2o::as.factor(train[,1])
test[,1] = h2o::as.factor(test[,1])
# 学習
model = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("Rectifier"),
hidden = c(10, 10), # 中間層のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
# hidden_dropout_ratios = 0.1,  # 中間層のdropout
hidden_dropout_ratios = rep(0.1, 10*10),
sparse = TRUE,      # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ効率を高める
seed = 0
)
rep(0.1, 10*10)
# 学習
model = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("RectifierWithDropout"),
hidden = c(10, 10), # 中間層のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
# hidden_dropout_ratios = 0.1,  # 中間層のdropout
hidden_dropout_ratios = rep(0.1, 0.1),
sparse = TRUE,      # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ効率を高める
seed = 0
)
# 学習
model = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train, # 学習用データを指定
activation = c("RectifierWithDropout"),
hidden = c(10, 10), # 中間層のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させるか。
# hidden_dropout_ratios = 0.1,  # 中間層のdropout
hidden_dropout_ratios = c(0.1, 0.1),
sparse = TRUE,      # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ効率を高める
seed = 0
)
plot.nn(titanic_nnet)
# 単一中間層
library(nnet)
plot.nn(titanic_nnet)
plot(titanic_nnet)
install.packages("NeuralNetTools")
library(NeuralNetTools)
plotnet(nn1)
plotnet(titanic_nnet)
plotnet(mnist_dl)
plotnet(model)
remove.packages("NeuralNetTools")
if(!require(NeuralNetTools)) {install.packages("NeuralNetTools")}
library(NeuralNetTools)
plotnet(titanic_nnet)
if(!require(NeuralNetTools)) {install.packages("NeuralNetTools")}
library(NeuralNetTools)
plotnet(titanic_nnet)
y_true = as.vector(test[,1])
y_pred = as.vector(pred[,1])
# 混同行列
table(y_true, y_pred)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred, y_true = y_true)
# 学習
mnist_dl = h2o.deeplearning(
x = 2:ncol(train),  # 特徴量の列番号を指定
y = 1,              # 目的変数の列番号を指定
training_frame = train,      # 学習用データを指定
activation = c("RectifierWithDropout"), # 活性化関数を指定
hidden = c(128, 64, 16),     # 中間層（隠れ層）のサイズ
epochs = 10,        # エポック数。学習データ何回分の学習を反復させて重みを更新していくか。
hidden_dropout_ratios = c(0.5, 0.5, 0.5),  # 各中間層でdropoutする割合
sparse = TRUE,      # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ使用量を抑える。
seed = 0
)
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(gridExtra)
library(tidyverse)
# 階段関数
step = function(x, b) ifelse(x >= -b, 1, 0)
# シグモイド関数
sigmoid = function(x) 1 / (1 + exp(-x))
# データ生成
x = -100:100 * 0.1
# plot
g1 <- ggplot(tibble(x, y = step(x, 0)),
aes(x = x, y = y))+
geom_line(color = "dodgerblue")+
labs(y = expression(y), x = expression(x), title = "階段関数")
g2 <- ggplot(tibble(x, y = sigmoid(x)),
aes(x = x, y = y))+
geom_line(color = "dodgerblue")+
labs(y = expression(y), x = expression(x), title = "シグモイド関数")
grid.arrange(g1, g2, ncol = 2)
# Chunk 3
# 準備
library(tidyverse)
set.seed(666)
# データ読み込み
library(carData)
data("TitanicSurvival")
head(TitanicSurvival)
# Chunk 4
# NA（欠損値）を含む行を削除
tita <- na.omit(TitanicSurvival)
# Chunk 5
# ID列を追加
df = tita %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 6
# 単一中間層
library(nnet)
titanic_nnet <- nnet(survived ~ . , data = train,
size = 2, decay = 0.1)
# Chunk 7
if(!require(NeuralNetTools)) {install.packages("NeuralNetTools")} # インストールされていなければインストールする
library(NeuralNetTools)
plotnet(titanic_nnet)
# Chunk 8
# 予測
y_pred_train = predict(titanic_nnet, train, type = "class")
# 混同行列
table(train$survived, y_pred_train)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_train, y_true = train$survived)
# Chunk 9
# 予測
y_pred_test = predict(titanic_nnet, test, type = "class")
# 混同行列
table(test$survived, y_pred_test)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_test, y_true = test$survived)
# Chunk 10
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)
# 乱数の種を固定
set.seed(0)
# Chunk 11
# MNISTデータのダウンロード
if (!dir.exists('data')) { # もしdataディレクトリがないなら作成
dir.create('data')
}
if (!file.exists('data/train.csv')) { # もしdataディレクトリにtrain.csvがないならダウンロード
download.file(url='https://raw.githubusercontent.com/wehrley/Kaggle-Digit-Recognizer/master/train.csv',
destfile='data/train.csv')
}
# Chunk 12
# データの読み込み
mnist <- read.csv('data/train.csv')
# Chunk 13
# 1レコード目の1~10列
mnist[1, 1:10]
# 1レコード目の210~220列
mnist[1, 210:220]
# 1レコード目の最後の10列
k = ncol(mnist)
mnist[1, (k-10):k]
# Chunk 14
# 教師データ（目的変数）= 1列目
y <- mnist[,1]
# 特徴量（説明変数）= 残りの784列
X <- mnist[,-1]
# 0から1の値になるよう正規化し、(数値が0~255なので、255で割る)、行と列をt()で入れ替える
X <- t(X/255)
# 扱っているデータを覗く
i = 10 # 10レコード目
pixels = matrix(X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((255:0)/255),
xlab="", ylab="", main=paste("Label for this image:", y[i]))
i = 1000 # 1000レコード目
pixels = matrix(X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((255:0)/255),
xlab="", ylab="", main=paste("Label for this image:", y[i]))
# Chunk 15
# testとtrainに分割
# ID列を追加
df = mnist %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 16
# 教師データのラベル
table(train[, 1])
# Chunk 17
# ライブラリの用意
if(!require(h2o)) {install.packages("h2o")}
library(h2o)
# 初期化
h2o.init()
# データをh2o用のデータ型に変換
train <- as.h2o(train)
test <- as.h2o(test)
# 1列目（目的変数）をfactor型に変換
train[,1] = h2o::as.factor(train[,1])
test[,1] = h2o::as.factor(test[,1])
# Chunk 18
# trainデータを学習用のものと検証用のものに分ける
splits <- h2o.splitFrame(train, ratios = 0.8, seed = 0)
# Chunk 19
# 学習
mnist_dl = h2o.deeplearning(
x = 2:ncol(train), # 特徴量の列番号を指定
y = 1,             # 目的変数の列番号を指定
training_frame = splits[[1]],   # 訓練データを指定
validation_frame = splits[[2]], # 検証データを指定（学習には使わず、精度を測るためだけに使う）
activation = c("RectifierWithDropout"), # 活性化関数を指定
hidden = c(64, 32, 16), # 中間層（隠れ層）のサイズ
epochs = 20,       # エポック数。学習データ何回分の学習を反復させて重みを更新していくか。
hidden_dropout_ratios = c(0.5, 0.5, 0.5), # 各中間層においてdropoutするユニットの割合
sparse = TRUE,     # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ使用量を抑える。
seed = 0
)
# Chunk 20
mnist_dl
# エポック数ごとの訓練データに対する予測誤差の推移
plot(mnist_dl)
mnist_dl
install.packages("DiagrammeR")
install.packages("DiagrammeR")
library(DiagrammeR)
install.packages("DiagrammeR")
library(glue)
install.packages("glue")
install.packages("DiagrammeR")
library(glue)
install.packages(c("backports", "bayestestR", "BH", "blob", "broom", "car", "carData", "caret", "class", "cli", "conquer", "corrplot", "crayon", "crosstalk", "data.table", "datawizard", "DBI", "digest", "DT", "dtplyr", "estimatr", "fansi", "flextable", "forecast", "foreign", "fs", "future", "generics", "ggsignif", "glmnet", "glue", "googlesheets4", "haven", "hms", "htmltools", "htmlwidgets", "insight", "ipred", "isoband", "jsonlite", "knitr", "later", "lattice", "lava", "lifecycle", "lmtest", "lubridate", "maptools", "MASS", "Matrix", "matrixStats", "mgcv", "mime", "modelsummary", "nlme", "nloptr", "nnet", "officer", "openssl", "openxlsx", "padr", "parallelly", "parameters", "performance", "pillar", "plotly", "pROC", "psych", "Quandl", "R6", "Rblpapi", "Rcpp", "RcppArmadillo", "readr", "recipes", "remotes", "repr", "reprex", "rio", "rJava", "rlang", "rmarkdown", "rsample", "Rttf2pt1", "rvest", "sp", "spatial", "stringi", "survival", "systemfonts", "tibble", "tidyr", "timetk", "tinytex", "tseries", "TTR", "utf8", "uuid", "withr", "xfun", "xml2"))
pacman::p_load(DiagrammeR)
