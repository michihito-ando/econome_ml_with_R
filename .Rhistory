fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("Pooled lm",
"Pooled plm",
"Oneway lm",
"Oneway plm",
"Twoway lm",
"Twoway plm"),
model.names = F, model.numbers = F)
# plmの固定効果モデル推定（個体固定効果と時間固定効果）
fe_twoway_plm <- plm(inv ~ value + capital,
data = panel_df, model = "within",
effect = "twoways") # within=固定効果モデル
# stargazerで比べる。
stargazer(fe_twoway_lm, fe_twoway_lm_robust, fe_twoway_plm, fe_twoway_plm_clustered,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("lm", "lm,robust", "plm", "plm,clustered"),
model.names = F,
model.numbers = F)
# lmで推定したものをロバスト標準誤差にする。
fe_twoway_lm_robust <- coeftest(fe_twoway_lm,
vcovHC(fe_twoway_lm, method = "arellano", type = "HC1"))
#plmで推定したものをクラスタロバスト標準誤差にする。
fe_twoway_plm_clustered <- coeftest(fe_twoway_plm,
vcovHC(fe_twoway_plm, method = "arellano", type = "HC1"))
# stargazerで比べる。
stargazer(fe_twoway_lm, fe_twoway_lm_robust, fe_twoway_plm, fe_twoway_plm_clustered,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("lm", "lm,robust", "plm", "plm,clustered"),
model.names = F,
model.numbers = F)
# lmで推定したものをロバスト標準誤差にする。
fe_twoway_lm_robust <- coeftest(fe_twoway_lm,
vcovHC(fe_twoway_lm, method = "arellano", type = "HC1"))
#plmで推定したものをクラスタロバスト標準誤差にする。
fe_twoway_plm_clustered <- coeftest(fe_twoway_plm,
vcovHC(fe_twoway_plm, method = "arellano", type = "HC1"))
# stargazerで比べる。
stargazer(fe_twoway_lm, fe_twoway_lm_robust, fe_twoway_plm, fe_twoway_plm_clustered,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("lm", "lm,robust", "plm", "plm,clustered"),
model.names = F,
model.numbers = F)
#stargazer
stargazer(pool_lm,
pool_plm,
fe_oneway_lm,
fe_oneway_plm,
fe_twoway_lm,
fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("Pooled lm",
"Pooled plm",
"Oneway lm",
"Oneway plm",
"Twoway lm",
"Twoway plm"),
model.names = F, model.numbers = F)
fe_oneway_lm <- lm(inv ~ value + capital + factor(firm), data = panel_df)
# plmによる固定効果モデル推定（個体固定効果のみ）
# yearも因子なので、factor(year)と書くと、ダミー変数として認識される。
fe_oneway_plm <- plm(inv ~ value + capital,
data = panel_df, model = "within") # within=固定効果モデル
#stargazer
stargazer(pool_lm, pool_plm, fe_oneway_lm, fe_oneway_plm,
type = "text", digits = 3, df = FALSE,
column.labels = c("Pooled lm", "Pooled plm", "Oneway lm", "Oneway plm"),
model.names = F, model.numbers = F)
#                  "Twoway plm"),
#fe_twoway_plm,
# plmの固定効果モデル推定（個体固定効果と時間固定効果）
fe_twoway_plm <- plm(inv ~ value + capital,
data = panel_df, model = "within",
effect = "twoways") # within=固定効果モデル
#stargazer
stargazer(pool_lm,
pool_plm,
fe_oneway_lm,
fe_oneway_plm,
fe_twoway_lm,
#fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("Pooled lm",
"Pooled plm",
"Oneway lm",
"Oneway plm",
"Twoway lm")
#                  "Twoway plm"),
model.names = F, model.numbers = F)
#stargazer
stargazer(pool_lm,
pool_plm,
fe_oneway_lm,
fe_oneway_plm,
fe_twoway_lm,
#fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("Pooled lm",
"Pooled plm",
"Oneway lm",
"Oneway plm",
"Twoway lm"),
#                  "Twoway plm"),
model.names = F, model.numbers = F)
#stargazer
stargazer(pool_lm,
pool_plm,
fe_oneway_lm,
fe_oneway_plm,
fe_twoway_lm,
fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("Pooled lm",
"Pooled plm",
"Oneway lm",
"Oneway plm",
"Twoway lm"),
"Twoway plm"),
model.names = F, model.numbers = F)
#stargazer
stargazer(pool_lm,
pool_plm,
fe_oneway_lm,
fe_oneway_plm,
fe_twoway_lm,
fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("Pooled lm",
"Pooled plm",
"Oneway lm",
"Oneway plm",
"Twoway lm",
"Twoway plm"),
model.names = F, model.numbers = F)
#stargazer
stargazer(pool_lm,
pool_plm,
fe_oneway_lm,
fe_oneway_plm,
fe_twoway_lm,
fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("Pooled lm",
"Pooled plm",
"Oneway lm",
"Oneway plm",
"Twoway lm",
"Twoway plm"),
model.names = F, model.numbers = F)
# plmの固定効果モデル推定（個体固定効果と時間固定効果）
fe_twoway_plm <- plm(inv ~ value + capital,
data = panel_df, model = "within",
effect = "twoways") # within=固定効果モデル
#stargazer
stargazer(#pool_lm,
pool_plm,
fe_oneway_lm,
fe_oneway_plm,
fe_twoway_lm,
fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c(#"Pooled lm",
"Pooled plm",
"Oneway lm",
"Oneway plm",
"Twoway lm",
"Twoway plm"),
model.names = F, model.numbers = F)
#結果表の出力
screenreg(list(pooled_lm, pooled_robust),
custom.model.names = c("Pooled", "Pooled, robust"),
include.ci = FALSE, digits = 3)
#stargazer
stargazer(fe_oneway_lm,
fe_oneway_plm,
fe_twoway_lm,
fe_twoway_plm,
type = "text",
digits = 3,
df = FALSE,
column.labels = c("Oneway lm",
"Oneway plm",
"Twoway lm",
"Twoway plm"),
model.names = F, model.numbers = F)
　　　　　　+ geom_line()
# 投資総額(Y)
Grunfeld %>% ggplot(aes(x = year,
y = inv,
colour = as.factor(firm))) +
　　　　　　geom_line()
pacman::p_load(readxl,
tidyverse,
AER,
estimatr,
skimr,
stargazer,
texreg)
data("CPS1985")
head(CPS1985)
#textでの表示
stargazer(CPS1985, type = "text")
#htmlで表示し、summary_CPS1985.htmlという名前での保存
stargazer(CPS1985, type = "html", out = "summary_CPS1985.html")
getwd()
presenter <- c( "Suzuki_ryo", "Suzuki_haru", "Narisako", "Murata", "Yoshida", "Yamamoto")
presenter <- c( "Suzuki_ryo", "Suzuki_haru", "Narisako", "Murata", "Yoshida", "Yamamoto")
#サンプル抽出
sample(presenter, 1)
presenter <- c( "Suzuki_ryo", "Suzuki_haru", "Narisako", "Murata", "Yoshida", "Yamamoto")
#サンプル抽出
sample(presenter, 1)
sample(discussant, 1)
sample(discussant2, 1)
#サンプル抽出
sample(presenter, 1)
#サンプル抽出
sample(presenter, 1)
#サンプル抽出
sample(presenter, 1)
#サンプル抽出
sample(presenter, 1)
#サンプル抽出
sample(presenter, 1)
#サンプル抽出
sample(presenter, 1)
pacman::p_load(tidyverse,
AER, # HousePricesデータを使用
stargazer, # lmやplmのときの結果表
MLmetrics, # 機械学習で用いる関数が実装
carData) # タイタニック号データを使用
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(tidyverse)
library(MLmetrics)
set.seed(0)
# データ生成 ----
n = 20
# x = rnorm(n, sd = 0.5)
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
df = tibble(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
df <- df %>% select(-ID)
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 3
ggplot(train, aes(x, y))+
geom_point()
# Chunk 4
# plot
p = 1
p1 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 1))+
labs(title = str_c(p,"次多項式回帰"))
p = 2
p2 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2))+
labs(title = str_c(p,"次多項式回帰"))
p = 3
p3 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 3))+
labs(title = str_c(p,"次多項式回帰"))
p = 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 5))+
labs(title = str_c(p,"次多項式回帰"))
p = 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 7))+
labs(title = str_c(p,"次多項式回帰"))
p = 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 9))+
labs(title = str_c(p,"次多項式回帰"))
library(gridExtra)
grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
# Chunk 5
# 次数を変えながら学習・予測
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- tibble(p, train_error, test_error) # p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, tibble(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# ggplotで多項式回帰と誤差(RMSE)の関係を描画
ggplot(result %>% gather(key, error, -p),
aes(x = p, y = error, color = key))+
geom_point()+
geom_line(size = 1)+
scale_x_continuous(breaks = c(1:9))+
labs(color = "", x = "多項式回帰の次数", y = "誤差（RMSE)",
title = "モデルの複雑さと過学習")
# Chunk 6
# パラメータ空間
# df_plot = expand.grid(beta_1 = -50:50 * 1/50,
#                      beta_2 = -50:50 * 1/50) %>%
#   mutate(beta_L2 = beta_1^2 + beta_2^2) %>%
#   mutate(isR = 1*(beta_L2 <= 0.5))
#
# ggplot(df_plot, aes(x = beta_1, y = beta_2, color = isR))+
#   geom_point()+
#   geom_hline(yintercept = 0)+
#   geom_vline(xintercept = 0)
# Chunk 7
# data
x1 <- rnorm(n = 100, mean = 5, sd = 3)
e <- rnorm(n = 100, mean = 5, sd = 6)
X <- tibble(x1,
x2 = 2*x1 + e)
# plot
g1 <- ggplot(X, aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "元のデータ")
g2 <- ggplot(scale(X) %>% data.frame(), aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "標準化後のデータ")
grid.arrange(g1,g2, ncol = 2)
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)
# 乱数の種を固定
set.seed(0)
# データ生成 ----
n = 20
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
# xの2乗～9乗の変数を作成
df = tibble(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# glmnetは説明変数Xと目的変数yを別々に入れる必要があるので分ける
# train
X_train = train %>% select(-y) %>% as.matrix() # as.matrix()でmatrix型のデータにする
y_train = train$y
# test
X_test = test %>% select(-y) %>% as.matrix()
y_test = test$y
install.packages("glmnet")
# 9次多項式でリッジ回帰
library(glmnet)
reg_ridge <- glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 0.1, # λ：任意の値
alpha = 0) # alpha = 0でリッジ回帰
# trainに対する予測精度
y_train_pred <- predict(reg_ridge, X_train) # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(reg_ridge, X_test) # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
# 結果を表示(str_c:複数のstringsを1つにまとめる)
str_c("test_error: ", round(test_error, 3)) %>% print()
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
# ラムダと交差検証誤差のグラフ
plot(ridge_cv)
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(ridge_cv, s = "lambda.min") %>% round(3)
# (参考)MSE+1SEの中で最大のラムダを採用したときの係数推定値
coef(ridge_cv, s = "lambda.1se") %>% round(3)
# trainに対する予測精度
y_train_pred <- predict(ridge_cv, X_train, s = "lambda.min") # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(ridge_cv, X_test, s = "lambda.min") # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
str_c("test_error: ", round(test_error, 3)) %>% print() # 結果を表示
# 後で使うので格納
y_test_pred_ridge <- y_test_pred
y_train_pred_ridge <- y_train_pred
# testへの9次多項式OLS
reg_ls <- lm(y ~ ., train)
y_test_pred_ls <- predict(reg_ls, test)
# data
data_for_plot <- tibble(x = test$x, y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred)) %>%
gather(model, value, -x,-y)
# plot
ggplot(data_for_plot, aes(x, y = value, color = model))+
geom_point(aes(x,y), size = 1.5, color = "black")+
geom_line(size = 1.5, alpha = 0.5)+
geom_point(size = 2, alpha = 0.5)+
scale_color_brewer(palette = "Set1")+
labs(title = "9次多項式回帰によるtestデータに対する予測")
# ggplot(test, aes(x, y))+
#   geom_point()+
#   geom_line(aes(y = y_test_pred_ls), color = "blue", size = 1, alpha = 0.7)+
#   geom_point(aes(y = y_test_pred_ls), color = "blue", size = 1, alpha = 0.7)+
#   geom_line(aes(y = y_test_pred), color = "orange", size = 1, alpha = 0.7)+
#   geom_point(aes(y = y_test_pred), color = "orange", size = 1, alpha = 0.7)+
#   labs(title = "testデータでの9次多項式回帰による予測")
ggplot(result %>% gather(key, error, -p),
aes(x = p, y = error, color = key))+
geom_point()+
geom_line(size = 1)+
scale_x_continuous(breaks = c(1:9))+
labs(color = "", x = "多項式回帰の次数", y = "誤差（RMSE)",
title = "モデルの複雑さと過学習（再掲）")
# ラムダを変えながら交差検証で誤差を評価
lasso_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
alpha = 1) # alpha = 1でlasso
# ラムダと交差検証MSEのグラフ
plot(lasso_cv)
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(lasso_cv, s = "lambda.min") %>% round(3)
# (参考)MSE+1SEの中で最大のラムダを採用したときの係数推定値
coef(lasso_cv, s = "lambda.1se") %>% round(3)
# trainに対する予測精度
y_train_pred <- predict(lasso_cv, X_train, s = "lambda.min") # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred = predict(lasso_cv, X_test, s = "lambda.min") # 予測
test_error = RMSE(y_test_pred, y_test) # 誤差(MSE)
str_c("test_error: ", round(test_error, 3)) %>% print() # 結果を表示
coef_df = cbind(LS = coef(reg_ls) %>% round(3),
Ridge = coef(ridge_cv, s = "lambda.min") %>% as.matrix() %>% round(3),
Lasso = coef(lasso_cv, s = "lambda.min") %>% as.matrix() %>% round(3)) %>% as.data.frame()
colnames(coef_df) = c("LS","Ridge","Lasso")
knitr::kable(coef_df)
# 予測値のオブジェクト名を一応変えておく
y_test_pred_lasso = y_test_pred
# data
data_for_plot <- tibble(x = test$x, y = test$y,
LS = y_test_pred_ls,
Ridge = c(y_test_pred_ridge),
Lasso = c(y_test_pred_lasso)) %>%
gather(model, value, -x,-y)
## 順番を指定
data_for_plot$model = factor(data_for_plot$model, levels = unique(data_for_plot$model))
# plot
ggplot(data_for_plot, aes(x, y = value, color = model))+
geom_point(aes(x,y), size = 1.5, color = "black")+
geom_line(size = 1.5, alpha = 0.5)+
geom_point(size = 2, alpha = 0.5)+
scale_color_brewer(palette = "Set1")+
labs(title = "9次多項式回帰によるtestデータに対する予測")
error_df = data.frame(LS = RMSE(y_test_pred_ls, y_test),
Ridge = RMSE(y_test_pred_ridge, y_test),
Lasso = RMSE(y_test_pred_lasso, y_test))
rownames(error_df) = "RMSE"
knitr::kable(error_df)
# Elastic Net
# ラムダを変えながら交差検証で誤差を評価
EN_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
alpha = 0.35) # 任意のα
# testに対する予測精度
y_test_pred = predict(EN_cv, X_test, s = "lambda.min") # 予測
test_error = RMSE(y_test_pred, y_test) # 誤差
str_c("test_error: ", round(test_error, 3)) %>% print() # 結果を表示
