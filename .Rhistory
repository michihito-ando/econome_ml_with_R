geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 1))+
labs(title = str_c(p,"次多項式回帰"))
p = 2
p2 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2))+
labs(title = str_c(p,"次多項式回帰"))
p = 3
p3 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 3))+
labs(title = str_c(p,"次多項式回帰"))
p = 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 5))+
labs(title = str_c(p,"次多項式回帰"))
p = 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 7))+
labs(title = str_c(p,"次多項式回帰"))
p = 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 9))+
labs(title = str_c(p,"次多項式回帰"))
library(gridExtra)
grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- data_frame(p, train_error, test_error)　# p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, data_frame(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# trainに対する予測精度
y_train_pred <- predict(ridge_cv, X_train, s = "lambda.min") # 予測
# ラムダを変えながら交差検証で誤差を評価
lasso_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
alpha = 1) # alpha = 1でlasso
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(lasso_cv, s = "lambda.min") %>% round(3)
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(tidyverse)
library(MLmetrics)
set.seed(0)
# データ生成 ----
n = 20
# x = rnorm(n, sd = 0.5)
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
df <- df %>% select(-ID)
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 3
ggplot(train, aes(x, y))+
geom_point()
# Chunk 4
# plot
p = 1
p1 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 1))+
labs(title = str_c(p,"次多項式回帰"))
p = 2
p2 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2))+
labs(title = str_c(p,"次多項式回帰"))
p = 3
p3 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 3))+
labs(title = str_c(p,"次多項式回帰"))
p = 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 5))+
labs(title = str_c(p,"次多項式回帰"))
p = 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 7))+
labs(title = str_c(p,"次多項式回帰"))
p = 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 9))+
labs(title = str_c(p,"次多項式回帰"))
library(gridExtra)
grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
# Chunk 5
# 次数を変えながら学習・予測
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- data_frame(p, train_error, test_error) # p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, data_frame(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# ggplotで多項式回帰と誤差(RMSE)の関係を描画
ggplot(result %>% gather(key, error, -p),
aes(x = p, y = error, color = key))+
geom_point()+
geom_line(size = 1)+
scale_x_continuous(breaks = c(1:9))+
labs(color = "", x = "多項式回帰の次数", y = "誤差（RMSE)",
title = "モデルの複雑さと過学習")
# Chunk 6
# パラメータ空間
# df_plot = expand.grid(beta_1 = -50:50 * 1/50,
#                      beta_2 = -50:50 * 1/50) %>%
#   mutate(beta_L2 = beta_1^2 + beta_2^2) %>%
#   mutate(isR = 1*(beta_L2 <= 0.5))
#
# ggplot(df_plot, aes(x = beta_1, y = beta_2, color = isR))+
#   geom_point()+
#   geom_hline(yintercept = 0)+
#   geom_vline(xintercept = 0)
# Chunk 7
# data
x1 <- rnorm(n = 100, mean = 5, sd = 3)
e <- rnorm(n = 100, mean = 5, sd = 6)
X <- data_frame(x1,
x2 = 2*x1 + e)
# plot
g1 <- ggplot(X, aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "元のデータ")
g2 <- ggplot(scale(X) %>% data.frame(), aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "標準化後のデータ")
grid.arrange(g1,g2, ncol = 2)
# Chunk 8
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)
# 乱数の種を固定
set.seed(0)
# データ生成 ----
n = 20
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
# xの2乗～9乗の変数を作成
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 9
# glmnetは説明変数Xと目的変数yを別々に入れる必要があるので分ける
# train
X_train = train %>% select(-y) %>% as.matrix() # as.matrix()でmatrix型のデータにする
y_train = train$y
# test
X_test = test %>% select(-y) %>% as.matrix()
y_test = test$y
# Chunk 11
# 9次多項式でリッジ回帰
library(glmnet)
reg_ridge <- glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 0.1, # λ：任意の値
alpha = 0) # alpha = 0でリッジ回帰
# trainに対する予測精度
y_train_pred <- predict(reg_ridge, X_train) # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(reg_ridge, X_test) # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
# 結果を表示(str_c:複数のstringsを1つにまとめる)
str_c("test_error: ", round(test_error, 3)) %>% print()
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
# ラムダと交差検証誤差のグラフ
plot(ridge_cv)
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
library(tidyverse)
library(MLmetrics)
set.seed(0)
# データ生成 ----
n = 20
# x = rnorm(n, sd = 0.5)
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
df <- df %>% select(-ID)
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 3
ggplot(train, aes(x, y))+
geom_point()
# Chunk 4
# plot
p = 1
p1 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 1))+
labs(title = str_c(p,"次多項式回帰"))
p = 2
p2 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 2))+
labs(title = str_c(p,"次多項式回帰"))
p = 3
p3 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 3))+
labs(title = str_c(p,"次多項式回帰"))
p = 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 5))+
labs(title = str_c(p,"次多項式回帰"))
p = 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 7))+
labs(title = str_c(p,"次多項式回帰"))
p = 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm", se = F, formula = y ~ poly(x, 9))+
labs(title = str_c(p,"次多項式回帰"))
library(gridExtra)
grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
# Chunk 5
# 次数を変えながら学習・予測
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- data_frame(p, train_error, test_error) # p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, data_frame(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# ggplotで多項式回帰と誤差(RMSE)の関係を描画
ggplot(result %>% gather(key, error, -p),
aes(x = p, y = error, color = key))+
geom_point()+
geom_line(size = 1)+
scale_x_continuous(breaks = c(1:9))+
labs(color = "", x = "多項式回帰の次数", y = "誤差（RMSE)",
title = "モデルの複雑さと過学習")
# Chunk 6
# パラメータ空間
# df_plot = expand.grid(beta_1 = -50:50 * 1/50,
#                      beta_2 = -50:50 * 1/50) %>%
#   mutate(beta_L2 = beta_1^2 + beta_2^2) %>%
#   mutate(isR = 1*(beta_L2 <= 0.5))
#
# ggplot(df_plot, aes(x = beta_1, y = beta_2, color = isR))+
#   geom_point()+
#   geom_hline(yintercept = 0)+
#   geom_vline(xintercept = 0)
# Chunk 7
# data
x1 <- rnorm(n = 100, mean = 5, sd = 3)
e <- rnorm(n = 100, mean = 5, sd = 6)
X <- data_frame(x1,
x2 = 2*x1 + e)
# plot
g1 <- ggplot(X, aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "元のデータ")
g2 <- ggplot(scale(X) %>% data.frame(), aes(x = x1, y = x2))+
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "標準化後のデータ")
grid.arrange(g1,g2, ncol = 2)
# Chunk 8
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)
# 乱数の種を固定
set.seed(0)
# データ生成 ----
n = 20
x = 1:n * 1/n
y = 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
# xの2乗～9乗の変数を作成
df = data_frame(y, x, x2 = x^2, x3 = x^3, x4 = x^4, x5 = x^5, x6 = x^6, x7 = x^7, x8 = x^8, x9 = x^9)
# データを分割 ----
# ID列を追加
df = df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 9
# glmnetは説明変数Xと目的変数yを別々に入れる必要があるので分ける
# train
X_train = train %>% select(-y) %>% as.matrix() # as.matrix()でmatrix型のデータにする
y_train = train$y
# test
X_test = test %>% select(-y) %>% as.matrix()
y_test = test$y
# Chunk 11
# 9次多項式でリッジ回帰
library(glmnet)
reg_ridge <- glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 0.1, # λ：任意の値
alpha = 0) # alpha = 0でリッジ回帰
# trainに対する予測精度
y_train_pred <- predict(reg_ridge, X_train) # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(reg_ridge, X_test) # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
# 結果を表示(str_c:複数のstringsを1つにまとめる)
str_c("test_error: ", round(test_error, 3)) %>% print()
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
)
# ラムダと交差検証誤差のグラフ
plot(ridge_cv)
colMeans(df2[2:3,c(2,3)]) # 2,3行の2,3列の平均を表示
colMeans(df2[2:3,-c(1)])
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center')
knitr::opts_chunk$set(message = F)
knitr::opts_chunk$set(warning = F)
# Chunk 2
123 + 456
# Chunk 3
x <- 1 + 1  # オブジェクト x に 1+1の演算結果を代入
# Chunk 4
x # オブジェクト x の中身を表示
# Chunk 5
# ベクトル
y <- c(1, 20, 100)
y
# Chunk 6
# 1~10の整数
x <- 1:10
x
# Chunk 7
z <- c("あ", "いう", "えおか") # これならエラーにならない
z
# Chunk 9
# 「あ」「いう」「えおか」をそれぞれ定義
あ <- "A"
いう <- "IU"
えおか <- "EOKA"
# これならエラーにならない
z <- c(あ, いう, えおか)
z
# Chunk 10
onetwothree <- c(1, "1", 2, "二", 3) #全部文字列(chr)になる
onetwothree
# Chunk 11
LETTERS
# Chunk 12
LETTERS[c(1, 12, 23)] #lLETTERSの1,12,23の要素抜出
# Chunk 13
#関数
sum(1:100) #1~100の合計
# Chunk 15
df1 <- data.frame(クラス = c("B","A","A"),
　　英語 = c(79, 91, 89),
数学 = c(75, 81, 92))
df1
# Chunk 16
#データフレームの操作
df1[1,] # 1行目を表示
df1[1:3, 1:3] #1-3行の1-3列を表示
df1[c(1,2,3), c(1,2,3)] #1-3行の1-3列を表示
df1[c(1,3), c(1,3)] #1,3行の1,3列を表示
# Chunk 17
df1["クラス"]
# Chunk 18
# ワークスペース
getwd()  # ワークスペースを調べる（get working directory）
setwd("./") # ワークスペースの場所を設定する
# "./"は現在のフォルダを指すのでsetwd("./") は実質なにも指定していない
# Chunk 19
#データをcsvファイルに書き出す
write.csv(df1, "df1.csv", row.names = FALSE)
# Chunk 20
#データをcsvファイルから読み込んでdf2という名前にする
df2 <- read.csv("df1.csv")
colMeans(df2[2:3,c(2,3)]) # 2,3行の2,3列の平均を表示
colMeans(df2[2:3,2:3)]) # 2,3行の2,3列の平均を表示
colMeans(df2[2:3,2:3)]) # 2,3行の2,3列の平均を表示
colMeans(df2[2:3,2:3]) # 2,3行の2,3列の平均を表示
colMeans(df2[2:3,-1])
#データフレームの操作
df1[1,] # 1行目を表示
df1[2] # 1行目を表示
colMeans(df2[2:3,-c(1)])
colMeans(df2[2:3,c(2,3)]) # 2,3行の2,3列の平均を表示
colMeans(df2[2:3,-c(1)])
colMeans(df2[2:3, -1])
colMeans(df2[1:3,c(1,3)]) # 1~3行の1,3列の平均を表示
colMeans(df2[1:3,c(1,3)]) # 1~3行の1,3列の平均を表示
colMeans(df2[1:3,c(2,3)]) # 1~3行の1,3列の平均を表示
colMeans(df2[1:3,c(2,4)]) # 1~3行の1,3列の平均を表示
colMeans(df2[2:3, 2:3)] # 1~3行の2~3列の平均を表示
colMeans(df2[2:3, 2:3] # 1~3行の2~3列の平均を表示
colMeans(df2[c(2,3), c(2,3)] # 1~3行の2~3列の平均を表示
colMeans(df2[2:3, 2:3] # 1~3行の2~3列の平均を表示
colMeans(df2[c(2,3), c(2,3)]) # 2,3行の2,3列の平均を表示
colMeans(df2[2:3, 2:3] # 1~3行の2~3列の平均を表示
colMeans(df2[2:3, 2:3]) # 1~3行の2~3列の平均を表示
colMeans(df2[2:3, 2:3]) # 1~3行の2~3列の平均を表示
colMeans(df2[c(2,3), c(2,3)]) # 2,3行の2,3列の平均を表示
colMeans(df2[2:3,-c(1)]) # 1~3行の2列目以外の平均を表示
colMeans(df2[2:3,-1]) # 1~3行の2列目以外の平均を表示
df1[ ,2] # 2列目を表示
df1[2] # 2列目を表示
df1[ ,2] # 2列目を表示
#データフレームの操作
df1[1, ] # 1行目を表示
df1[ ,2] # 2列目を表示
df1[1] # 2列目を表示
df1[ ,1] # 2列目を表示
df1[2] # 2列目を表示
df1[ ,2] # 2列目を表示
x <- df1[2]
x
x
x <- df1[, 2]
x
x <- df1[, 2]
x
df1[2] # 2列目の要素を表示
df1[ ,2] # 2列目を表示
#データフレームの操作
df1[1, ] # 1行目を表示
#データフレームの操作
df1[1, ] # 1行目を表示
df1[ ,2] # 2列目を表示
df1[2] # 2列目を表示
x <- df1[, 2]
x <- df1[2]
#データフレームの操作
df1[1, ] # 1行目を表示
#データフレームの操作
df1[1, ] # 1行目を表示
#データフレームの操作
df1[1, ] # 1行目を表示
#データフレームの操作
df1[1, ] # 1行目を表示
df1[ ,2] # 2列目の数字を表示
df1[2] # 2列目をデータフレームとして表示
df1[1:3, 1:3] #1~3行の1~3列を表示
df1[c(1,2,3), c(1,2,3)] #1,2,3行の1,2,3列を表示
df1[c(1,3), c(1,3)] #1,3行の1,3列を表示
