gridExtra, # グラフ結合用（今回は未使用）
glmnet) # lassoやリッジ用
# Chunk 3
# seedを固定
set.seed(0)
# データ生成 ----
n <- 50
# x = rnorm(n, sd = 0.5)
x <- 1:n * 1/n
y <- 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
df <- tibble(y,
x,
x2 = x^2,
x3 = x^3,
x4 = x^4,
x5 = x^5,
x6 = x^6,
x7 = x^7,
x8 = x^8,
x9 = x^9)
# データを分割 ----
# ID列を追加
df <- df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
df <- df %>% dplyr::select(-ID)
train <- train %>% dplyr::select(-ID)
test <- test %>% dplyr::select(-ID)
# Chunk 4
ggplot(train, aes(x, y))+
geom_point()
# Chunk 5
# plot
p <- 1
p1 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 1)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 2
p2 <- ggplot(train, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 2)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 3
p3 <- ggplot(train, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 3)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 5)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 7)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 9)) +
labs(title = str_c(p,"次多項式回帰"))
# patchworkを使ってグラフ結合
(p1 + p2 + p3) / (p5 + p7 + p9)
#grid.arrange()を使ってもいい
#library(gridExtra) # 複数のグラフをまとめるためのパッケージ
#grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
# Chunk 6
# 次数を変えながら学習・予測
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- tibble(p, train_error, test_error) # p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, tibble(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# ggplotで多項式回帰と誤差(RMSE)の関係を描画
ggplot(result %>% gather(key, error, -p),
aes(x = p,
y = error,
color = key)) +
geom_point() +
geom_line(size = 1) +
scale_x_continuous(breaks = c(1:9)) +
labs(color = "",
x = "多項式回帰の次数",
y = "誤差（RMSE)",
title = "モデルの複雑さと過学習")
# Chunk 7
# パラメータ空間
# df_plot = expand.grid(beta_1 = -50:50 * 1/50,
#                      beta_2 = -50:50 * 1/50) %>%
#   mutate(beta_L2 = beta_1^2 + beta_2^2) %>%
#   mutate(isR = 1*(beta_L2 <= 0.5))
#
# ggplot(df_plot, aes(x = beta_1, y = beta_2, color = isR))+
#   geom_point()+
#   geom_hline(yintercept = 0)+
#   geom_vline(xintercept = 0)
# Chunk 8
# data
x1 <- rnorm(n = 100, mean = 5, sd = 3)
e <- rnorm(n = 100, mean = 5, sd = 6)
X <- tibble(x1,
x2 = 2*x1 + e)
# plot
g1 <- ggplot(X, aes(x = x1, y = x2)) +
geom_point() +
geom_vline(xintercept = 0) +
geom_hline(yintercept = 0) +
labs(title = "元のデータ")
g2 <- ggplot(scale(X) %>% data.frame(), aes(x = x1, y = x2)) +
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "標準化後のデータ")
#patchworkを使ってグラフ結合
g1 + g2
#grid.arrange(g1,g2, ncol = 2)でもよい
# Chunk 9
# 乱数の種を固定
set.seed(0)
# データ生成 ----
n <- 50
x <- 1:n * 1/n
y <- 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
# xの2乗～9乗の変数を作成
df <- tibble(y,
x,
x2 = x^2,
x3 = x^3,
x4 = x^4,
x5 = x^5,
x6 = x^6,
x7 = x^7,
x8 = x^8,
x9 = x^9)
# データを分割 ----
# ID列を追加
df <- df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 10
# glmnetは説明変数Xと目的変数yを別々に入れる必要があるので分ける
# train
X_train <- train %>% select(-y) %>% as.matrix() # as.matrix()でmatrix型のデータにする
y_train <- train$y
# test
X_test <- test %>% select(-y) %>% as.matrix()
y_test <- test$y
# Chunk 11
# 9次多項式でリッジ回帰
reg_ridge <- glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 0.1, # λ：任意の値
alpha = 0) # alpha = 0でリッジ回帰
# trainに対する予測精度
y_train_pred <- predict(reg_ridge, X_train) # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(reg_ridge, X_test) # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
# 結果を表示(str_c:複数のstringsを1つにまとめる)
str_c("test_error: ", round(test_error, 3)) %>% print()
# Chunk 12
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
# ラムダと交差検証誤差のグラフ
plot(ridge_cv)
# Chunk 13
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(ridge_cv, s = "lambda.min") %>% round(3)
# (参考)MSE+1SEの中で最大のラムダを採用したときの係数推定値
coef(ridge_cv, s = "lambda.1se") %>% round(3)
# Chunk 14
# trainに対する予測精度
y_train_pred <- predict(ridge_cv,
X_train,
s = "lambda.min") # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(ridge_cv,
X_test,
s = "lambda.min") # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
str_c("test_error: ", round(test_error, 3)) %>% print() # 結果を表示
# Chunk 15
# 後で使うので格納
y_test_pred_ridge <- y_test_pred
y_train_pred_ridge <- y_train_pred
# Chunk 16
# testへの9次多項式OLS
reg_ls <- lm(y ~ ., train)
y_test_pred_ls <- predict(reg_ls, test)
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred))
#long形式に変換
data_for_plot <- data_for_plot %>%
tidyr::pivot_longer(OLS:Ridge,
names_to = "model",
values_to = "value")
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model))　+
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1")+
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
# Chunk 17
ggplot(result %>% gather(key, error, -p),
aes(x = p,
y = error,
color = key)) +
geom_point() +
geom_line(size = 1) +
scale_x_continuous(breaks = c(1:9))+
labs(color = "",
x = "多項式回帰の次数",
y = "誤差（RMSE)",
title = "モデルの複雑さと過学習（再掲）")
# Chunk 18
# ラムダを変えながら交差検証で誤差を評価
lasso_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
alpha = 1) # alpha = 1でlasso
# ラムダと交差検証MSEのグラフ
plot(lasso_cv)
# Chunk 19
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(lasso_cv, s = "lambda.min") %>% round(3)
# Chunk 20
# (参考)MSE+1SEの中で最大のラムダを採用したときの係数推定値
coef(lasso_cv, s = "lambda.1se") %>% round(3)
# Chunk 21
# trainに対する予測精度
y_train_pred <- predict(lasso_cv,
X_train,
s = "lambda.min") # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(lasso_cv,
X_test,
s = "lambda.min") # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差(MSE)
str_c("test_error: ", round(test_error, 3)) %>% print() # 結果を表示
# Chunk 22
coef_df <- cbind(OLS = coef(reg_ls) %>% round(3),
Ridge = coef(ridge_cv,
s = "lambda.min") %>%
as.matrix() %>% round(3),
Lasso = coef(lasso_cv,
s = "lambda.min") %>%
as.matrix() %>% round(3)) %>%
as.data.frame()
colnames(coef_df) <- c("OLS","Ridge","Lasso")
knitr::kable(coef_df)
# 予測値のオブジェクト名を一応変えておく
y_test_pred_lasso <- y_test_pred
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred_ridge),
Lasso = c(y_test_pred_lasso))　
# long形式に変換
data_for_plot <- data_for_plot %>% tidyr::pivot_longer(Ridge:Lasso,
names_to = "model",
values_to = "value")
#旧式のgather()を使う場合は以下のようになる
#data_for_plot <- data_for_plot %>% gather(model, value, -x,-y)
## 順番を指定
data_for_plot$model <- factor(data_for_plot$model,
levels = unique(data_for_plot$model))
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model)) +
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1") +
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
# long形式に変換
data_for_plot <- data_for_plot %>% tidyr::pivot_longer(OLS:Lasso,
names_to = "model",
values_to = "value")
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred_ridge),
Lasso = c(y_test_pred_lasso))　
# long形式に変換
data_for_plot <- data_for_plot %>% tidyr::pivot_longer(OLS:Lasso,
names_to = "model",
values_to = "value")
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred_ridge),
Lasso = c(y_test_pred_lasso))　
# long形式に変換
data_for_plot <- data_for_plot %>% tidyr::pivot_longer(OLS:Lasso,
names_to = "model",
values_to = "value")
## 順番を指定
data_for_plot$model <- factor(data_for_plot$model,
levels = unique(data_for_plot$model))
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model)) +
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1") +
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
# 準備
library(tidyverse)
set.seed(666)
# 準備
library(tidyverse)
set.seed(666)
# データ読み込み
library(carData)
data("TitanicSurvival")
head(TitanicSurvival)
# 準備
library(tidyverse)
set.seed(666)
# データ読み込み
library(carData)
data("TitanicSurvival")
head(TitanicSurvival)
# NA（欠損値）を含む行を削除
tita <- na.omit(TitanicSurvival)
# NA（欠損値）を含む行を削除
tita <- na.omit(TitanicSurvival)
# ID列を追加
df = tita %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# ID列を追加
df <- tita %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# 単一中間層
library(nnet)
titanic_nnet <- nnet(survived ~ . , data = train,
size = 2, decay = 0.1)
if(!require(NeuralNetTools)) {install.packages("NeuralNetTools")} # インストールされていなければインストールする
library(NeuralNetTools)
plotnet(titanic_nnet)
# 予測
y_pred_train = predict(titanic_nnet, train, type = "class")
# 混同行列
table(train$survived, y_pred_train)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_train, y_true = train$survived)
# 予測
y_pred_test = predict(titanic_nnet, test, type = "class")
# 混同行列
table(test$survived, y_pred_test)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred_test, y_true = test$survived)
# パッケージの読み込み
library(tidyverse)
library(MLmetrics)
# 乱数の種を固定
set.seed(0)
# MNISTデータのダウンロード
if (!dir.exists('data')) { # もしdataディレクトリがないなら作成
dir.create('data')
}
# データの読み込み
mnist <- read.csv('data/train.csv')
# 1レコード目の1~10列
mnist[1, 1:10]
# 1レコード目の210~220列
mnist[1, 210:220]
# 1レコード目の最後の10列
k = ncol(mnist)
# 教師データ（目的変数）= 1列目
y <- mnist[,1]
# 特徴量（説明変数）= 残りの784列
X <- mnist[,-1]
# 0から1の値になるよう正規化し、(数値が0~255なので、255で割る)、行と列をt()で入れ替える
X <- t(X/255)
# 扱っているデータを覗く
i = 10 # 10レコード目
pixels = matrix(X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((255:0)/255),
xlab="", ylab="", main=paste("Label for this image:", y[i]))
i = 1000 # 1000レコード目
pixels = matrix(X[,i], nrow=28, byrow=TRUE)
image(t(apply(pixels, 2, rev)) , col=gray((255:0)/255),
xlab="", ylab="", main=paste("Label for this image:", y[i]))
# testとtrainに分割
# ID列を追加
df = mnist %>% rownames_to_column("ID")
# 80%を学習用データに
train <- df %>% sample_frac(size = 0.8)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列は予測に使わないため削除しておく
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# 教師データのラベル
table(train[, 1])
# ライブラリの用意
if(!require(h2o)) {install.packages("h2o")}
library(h2o)
# データをh2o用のデータ型に変換
train <- as.h2o(train)
# 初期化
h2o.init()
# データをh2o用のデータ型に変換
train <- as.h2o(train)
test <- as.h2o(test)
# 1列目（目的変数）をfactor型に変換
train[,1] = h2o::as.factor(train[,1])
test[,1] = h2o::as.factor(test[,1])
# trainデータを学習用のものと検証用のものに分ける
splits <- h2o.splitFrame(train, ratios = 0.8, seed = 0)
# 学習
mnist_dl = h2o.deeplearning(
x = 2:ncol(train), # 特徴量の列番号を指定
y = 1,             # 目的変数の列番号を指定
training_frame = splits[[1]],   # 訓練データを指定
validation_frame = splits[[2]], # 検証データを指定（学習には使わず、精度を測るためだけに使う）
activation = c("RectifierWithDropout"), # 活性化関数を指定
hidden = c(128, 64, 16), # 中間層（隠れ層）のサイズ
epochs = 15,       # エポック数。学習データ何回分の学習を反復させて重みを更新していくか。
hidden_dropout_ratios = c(0.5, 0.5, 0.5), # 各中間層においてdropoutするユニットの割合
sparse = TRUE,     # 0が多いデータ（スパースデータ）の取り扱い方を変え、メモリ使用量を抑える。
standardize = TRUE,# データの標準化を学習前に行う
seed = 0
)
mnist_dl
# 予測誤差の推移
plot(mnist_dl)
# 予測
pred <- h2o.predict(mnist_dl, test)
# 予測結果
pred
# 予測確率
round(pred[,2:11], 3)
y_true = as.vector(test[,1])
y_pred = as.vector(pred[,1])
# 混同行列
table(y_true, y_pred)
# 混同行列（h2oパッケージの関数を使う場合）
h2o.confusionMatrix(mnist_dl, test)
# 正解率
library(MLmetrics)
Accuracy(y_pred = y_pred, y_true = y_true)
