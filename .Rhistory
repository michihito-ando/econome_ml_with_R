# ID列を追加
df <- df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 10
# glmnetは説明変数Xと目的変数yを別々に入れる必要があるので分ける
# train
X_train <- train %>% select(-y) %>% as.matrix() # as.matrix()でmatrix型のデータにする
y_train <- train$y
# test
X_test <- test %>% select(-y) %>% as.matrix()
y_test <- test$y
# Chunk 11
# 9次多項式でリッジ回帰
reg_ridge <- glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 0.1, # λ：任意の値
alpha = 0) # alpha = 0でリッジ回帰
# trainに対する予測精度
y_train_pred <- predict(reg_ridge, X_train) # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(reg_ridge, X_test) # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
# 結果を表示(str_c:複数のstringsを1つにまとめる)
str_c("test_error: ", round(test_error, 3)) %>% print()
# Chunk 12
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
# ラムダと交差検証誤差のグラフ
plot(ridge_cv)
# Chunk 13
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(ridge_cv, s = "lambda.min") %>% round(3)
# (参考)MSE+1SEの中で最大のラムダを採用したときの係数推定値
coef(ridge_cv, s = "lambda.1se") %>% round(3)
# Chunk 14
# trainに対する予測精度
y_train_pred <- predict(ridge_cv,
X_train,
s = "lambda.min") # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(ridge_cv,
X_test,
s = "lambda.min") # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
str_c("test_error: ", round(test_error, 3)) %>% print() # 結果を表示
# Chunk 15
# 後で使うので格納
y_test_pred_ridge <- y_test_pred
y_train_pred_ridge <- y_train_pred
# testへの9次多項式OLS
reg_ls <- lm(y ~ ., train)
y_test_pred_ls <- predict(reg_ls, test)
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls
Ridge = c(y_test_pred))
#long形式に変換
data_for_plot <- data_for_plot %>%
tidyr::pivot_longer(OLS:Ridge,
names_to = "model",
values_to = "value")
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model))　+
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1")+
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
# testへの9次多項式OLS
reg_ls <- lm(y ~ ., train)
y_test_pred_ls <- predict(reg_ls, test)
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls
Ridge = c(y_test_pred))
#long形式に変換
data_for_plot <- data_for_plot %>%
tidyr::pivot_longer(OLS:Ridge,
names_to = "model",
values_to = "value")
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model))　+
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1")+
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
# testへの9次多項式OLS
reg_ls <- lm(y ~ ., train)
y_test_pred_ls <- predict(reg_ls, test)
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred))
#long形式に変換
data_for_plot <- data_for_plot %>%
tidyr::pivot_longer(OLS:Ridge,
names_to = "model",
values_to = "value")
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model))　+
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1")+
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
# Chunk 1
knitr::opts_chunk$set(fig.align = 'center', message = F, warning = F)
# Chunk 2
pacman::p_load(tidyverse,
MLmetrics, # 機械学習で用いる関数が実装
patchwork, # グラフ結合用
gridExtra, # グラフ結合用（今回は未使用）
glmnet) # lassoやリッジ用
# Chunk 3
# seedを固定
set.seed(0)
# データ生成 ----
n <- 50
# x = rnorm(n, sd = 0.5)
x <- 1:n * 1/n
y <- 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
df <- tibble(y,
x,
x2 = x^2,
x3 = x^3,
x4 = x^4,
x5 = x^5,
x6 = x^6,
x7 = x^7,
x8 = x^8,
x9 = x^9)
# データを分割 ----
# ID列を追加
df <- df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
df <- df %>% dplyr::select(-ID)
train <- train %>% dplyr::select(-ID)
test <- test %>% dplyr::select(-ID)
# Chunk 4
ggplot(train, aes(x, y))+
geom_point()
# Chunk 5
# plot
p <- 1
p1 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 1)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 2
p2 <- ggplot(train, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 2)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 3
p3 <- ggplot(train, aes(x, y)) +
geom_point() +
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 3)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 5
p5 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 5)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 7
p7 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 7)) +
labs(title = str_c(p,"次多項式回帰"))
p <- 9
p9 <- ggplot(train, aes(x, y))+
geom_point()+
geom_smooth(method = "lm",
se = F,
formula = y ~ poly(x, 9)) +
labs(title = str_c(p,"次多項式回帰"))
# patchworkを使ってグラフ結合
(p1 + p2 + p3) / (p5 + p7 + p9)
#grid.arrange()を使ってもいい
#library(gridExtra) # 複数のグラフをまとめるためのパッケージ
#grid.arrange(grobs = list(p1, p2, p3, p5, p7, p9), ncol = 3)
# Chunk 6
# 次数を変えながら学習・予測
for(p in 1:9){
reg <- lm(y ~ ., data = train[,1:(p+1)]) # 学習
# trainへの予測精度
y_train_pred <- predict(reg, train[,1:(p+1)])   # 予測
train_error <- RMSE(y_train_pred, train$y) # 誤差
# testへの予測精度
y_test_pred <- predict(reg, test[,1:(p+1)])   # 予測
test_error <- RMSE(y_test_pred, test$y) # 誤差
if(p == 1) result <- tibble(p, train_error, test_error) # p=1の場合、データフレーム作る
if(p != 1) result <- rbind(result, tibble(p, train_error, test_error)) # p=2以上の場合はrbindで繋げる
}
# ggplotで多項式回帰と誤差(RMSE)の関係を描画
ggplot(result %>% gather(key, error, -p),
aes(x = p,
y = error,
color = key)) +
geom_point() +
geom_line(size = 1) +
scale_x_continuous(breaks = c(1:9)) +
labs(color = "",
x = "多項式回帰の次数",
y = "誤差（RMSE)",
title = "モデルの複雑さと過学習")
# Chunk 7
# パラメータ空間
# df_plot = expand.grid(beta_1 = -50:50 * 1/50,
#                      beta_2 = -50:50 * 1/50) %>%
#   mutate(beta_L2 = beta_1^2 + beta_2^2) %>%
#   mutate(isR = 1*(beta_L2 <= 0.5))
#
# ggplot(df_plot, aes(x = beta_1, y = beta_2, color = isR))+
#   geom_point()+
#   geom_hline(yintercept = 0)+
#   geom_vline(xintercept = 0)
# Chunk 8
# data
x1 <- rnorm(n = 100, mean = 5, sd = 3)
e <- rnorm(n = 100, mean = 5, sd = 6)
X <- tibble(x1,
x2 = 2*x1 + e)
# plot
g1 <- ggplot(X, aes(x = x1, y = x2)) +
geom_point() +
geom_vline(xintercept = 0) +
geom_hline(yintercept = 0) +
labs(title = "元のデータ")
g2 <- ggplot(scale(X) %>% data.frame(), aes(x = x1, y = x2)) +
geom_point()+
geom_vline(xintercept = 0)+
geom_hline(yintercept = 0)+
labs(title = "標準化後のデータ")
#patchworkを使ってグラフ結合
g1 + g2
#grid.arrange(g1,g2, ncol = 2)でもよい
# Chunk 9
# 乱数の種を固定
set.seed(0)
# データ生成 ----
n <- 50
x <- 1:n * 1/n
y <- 0.5 + 0.4*sin(2*pi*x) + rnorm(n = n, sd = 0.1)
# xの2乗～9乗の変数を作成
df <- tibble(y,
x,
x2 = x^2,
x3 = x^3,
x4 = x^4,
x5 = x^5,
x6 = x^6,
x7 = x^7,
x8 = x^8,
x9 = x^9)
# データを分割 ----
# ID列を追加
df <- df %>% rownames_to_column("ID")
# 50%を学習用データに
train <- df %>% sample_frac(size = 0.5)
# 学習用データに使っていないIDの行をテスト用データに
test <- anti_join(df, train, by = "ID")
# ID列の削除
train <- train %>% select(-ID)
test <- test %>% select(-ID)
# Chunk 10
# glmnetは説明変数Xと目的変数yを別々に入れる必要があるので分ける
# train
X_train <- train %>% select(-y) %>% as.matrix() # as.matrix()でmatrix型のデータにする
y_train <- train$y
# test
X_test <- test %>% select(-y) %>% as.matrix()
y_test <- test$y
# Chunk 11
# 9次多項式でリッジ回帰
reg_ridge <- glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 0.1, # λ：任意の値
alpha = 0) # alpha = 0でリッジ回帰
# trainに対する予測精度
y_train_pred <- predict(reg_ridge, X_train) # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(reg_ridge, X_test) # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
# 結果を表示(str_c:複数のstringsを1つにまとめる)
str_c("test_error: ", round(test_error, 3)) %>% print()
# Chunk 12
# ラムダを変えながら交差検証で誤差を評価
ridge_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
lambda = 1:100 * 0.00001, # 探索する範囲を指定（任意）
alpha = 0) # alpha = 0でリッジ回帰
# ラムダと交差検証誤差のグラフ
plot(ridge_cv)
# Chunk 13
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(ridge_cv, s = "lambda.min") %>% round(3)
# (参考)MSE+1SEの中で最大のラムダを採用したときの係数推定値
coef(ridge_cv, s = "lambda.1se") %>% round(3)
# Chunk 14
# trainに対する予測精度
y_train_pred <- predict(ridge_cv,
X_train,
s = "lambda.min") # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(ridge_cv,
X_test,
s = "lambda.min") # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差
str_c("test_error: ", round(test_error, 3)) %>% print() # 結果を表示
# Chunk 15
# 後で使うので格納
y_test_pred_ridge <- y_test_pred
y_train_pred_ridge <- y_train_pred
# Chunk 16
# testへの9次多項式OLS
reg_ls <- lm(y ~ ., train)
y_test_pred_ls <- predict(reg_ls, test)
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred))
#long形式に変換
data_for_plot <- data_for_plot %>%
tidyr::pivot_longer(OLS:Ridge,
names_to = "model",
values_to = "value")
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model))　+
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1")+
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
# Chunk 17
ggplot(result %>% gather(key, error, -p),
aes(x = p,
y = error,
color = key)) +
geom_point() +
geom_line(size = 1) +
scale_x_continuous(breaks = c(1:9))+
labs(color = "",
x = "多項式回帰の次数",
y = "誤差（RMSE)",
title = "モデルの複雑さと過学習（再掲）")
# Chunk 18
# ラムダを変えながら交差検証で誤差を評価
lasso_cv <- cv.glmnet(x = X_train, # 説明変数
y = y_train, # 目的変数
alpha = 1) # alpha = 1でlasso
# ラムダと交差検証MSEのグラフ
plot(lasso_cv)
# Chunk 19
# 誤差(MSE)を最小にしたラムダを採択したときの係数推定値
coef(lasso_cv, s = "lambda.min") %>% round(3)
# Chunk 20
# (参考)MSE+1SEの中で最大のラムダを採用したときの係数推定値
coef(lasso_cv, s = "lambda.1se") %>% round(3)
# Chunk 21
# trainに対する予測精度
y_train_pred <- predict(lasso_cv,
X_train,
s = "lambda.min") # 予測
train_error <- RMSE(y_train_pred, y_train) # 誤差
str_c("train_error: ", round(train_error, 3)) %>% print() # 結果を表示
# testに対する予測精度
y_test_pred <- predict(lasso_cv,
X_test,
s = "lambda.min") # 予測
test_error <- RMSE(y_test_pred, y_test) # 誤差(MSE)
str_c("test_error: ", round(test_error, 3)) %>% print() # 結果を表示
# Chunk 22
coef_df <- cbind(OLS = coef(reg_ls) %>% round(3),
Ridge = coef(ridge_cv,
s = "lambda.min") %>%
as.matrix() %>% round(3),
Lasso = coef(lasso_cv,
s = "lambda.min") %>%
as.matrix() %>% round(3)) %>%
as.data.frame()
colnames(coef_df) <- c("OLS","Ridge","Lasso")
knitr::kable(coef_df)
# 予測値のオブジェクト名を一応変えておく
y_test_pred_lasso <- y_test_pred
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred_ridge),
Lasso = c(y_test_pred_lasso))　
# long形式に変換
data_for_plot <- data_for_plot %>% tidyr::pivot_longer(Ridge:Lasso,
names_to = "model",
values_to = "value")
#旧式のgather()を使う場合は以下のようになる
#data_for_plot <- data_for_plot %>% gather(model, value, -x,-y)
## 順番を指定
data_for_plot$model <- factor(data_for_plot$model,
levels = unique(data_for_plot$model))
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model)) +
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1") +
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
# long形式に変換
data_for_plot <- data_for_plot %>% tidyr::pivot_longer(OLS:Lasso,
names_to = "model",
values_to = "value")
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred_ridge),
Lasso = c(y_test_pred_lasso))　
# long形式に変換
data_for_plot <- data_for_plot %>% tidyr::pivot_longer(OLS:Lasso,
names_to = "model",
values_to = "value")
# data
data_for_plot <- tibble(x = test$x,
y = test$y,
OLS = y_test_pred_ls,
Ridge = c(y_test_pred_ridge),
Lasso = c(y_test_pred_lasso))　
# long形式に変換
data_for_plot <- data_for_plot %>% tidyr::pivot_longer(OLS:Lasso,
names_to = "model",
values_to = "value")
## 順番を指定
data_for_plot$model <- factor(data_for_plot$model,
levels = unique(data_for_plot$model))
# plot
data_for_plot %>% ggplot(aes(x = x,
y = value,
color = model)) +
geom_point(aes(x = x,
y = y),
size = 1.5,
color = "black") +
geom_line(size = 1.5, alpha = 0.5) +
geom_point(size = 2, alpha = 0.5) +
scale_color_brewer(palette = "Set1") +
labs(title = "9次多項式回帰によるtestデータに対する予測",
y = "y and predicted y")
